{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "69b9a648-bcc7-490d-9f9b-ea244d156bd6"
   },
   "source": [
    "# Web Scraping from Indeed.com & Predicting Salaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "34681254-c802-462f-829d-8894d0772d08"
   },
   "source": [
    "This project includes collecting data by scraping Indeed.com and then building a binary classifier.\n",
    "\n",
    "We collect salary information on data science jobs in a variety of markets. Then using the location, title, and summary of the job we use listings with salary information to build a model and predict additional salaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "a948d79c-5527-4c0d-ab23-f5d43ce72056"
   },
   "source": [
    "## Scraping job listings from Indeed.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import re    # string manipulations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to extract location, company, job title, and salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_title_from_result(result):\n",
    "# find the title, if get error, return NONE\n",
    "    try:\n",
    "        result_txt = result.find('a',attrs={'data-tn-element':'jobTitle'}).text\n",
    "    except:\n",
    "        result_txt='NONE'\n",
    "    return result_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_company_from_result(result):\n",
    "# find the company, if get error, return NONE\n",
    "    try:\n",
    "        result_txt = result.find('span',class_='company').text\n",
    "    except:\n",
    "        result_txt='NONE'\n",
    "    result_txt= re.sub('\\n', '', result_txt).strip()   # strip '\\n' and leading blanks\n",
    "    return result_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_location_from_result(result):\n",
    "# find the location, if get error, return NONE\n",
    "    try:\n",
    "        result_txt = result.find('span',class_='location').text\n",
    "    except:\n",
    "        location_txt='NONE'\n",
    "    return result_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_salary_from_result(result):\n",
    "# find the salary, if get error, return NONE\n",
    "    try:\n",
    "        result_txt = result.find('span',attrs={'class':'no-wrap'}).text\n",
    "    except:\n",
    "        result_txt='NONE'\n",
    "    return result_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_description_from_result(result):\n",
    "# find the description, if get error, return NONE\n",
    "    try:\n",
    "        result_txt = result.find('span',attrs={'class':'summary'}).text\n",
    "        result_txt= re.sub('\\n', '', result_txt).strip()   # strip '\\n' and leading blanks\n",
    "    except:\n",
    "        result_txt='NONE'\n",
    "    return result_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape Indeed Data Science jobs in multiple cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Base URL to scrape from\n",
    "url_template = \"http://www.indeed.com/jobs?q=data+scientist+%2420%2C000&l={}&start={}\"\n",
    "\n",
    "max_results_per_city = 2000\n",
    "\n",
    "# Initialize lists\n",
    "titles=[]\n",
    "companies=[]\n",
    "locations=[]\n",
    "salaries=[]\n",
    "descriptions=[]\n",
    "\n",
    "# Loop to perform scraping by city\n",
    "for city in set(['New+York', 'Chicago', 'San+Francisco', 'Austin', 'Seattle', \n",
    "    'Los+Angeles', 'Philadelphia', 'Atlanta', 'Dallas', 'Pittsburgh', \n",
    "    'Portland', 'Phoenix', 'Denver', 'Houston', 'Miami', 'Raleigh']):\n",
    "\n",
    "    print city   # so can see status\n",
    "# Loop to perform scraping by page within each city\n",
    "    for start in range(0, max_results_per_city, 10):\n",
    "                \n",
    "        print start # so can see status\n",
    "        print('Timestamp: {:%Y-%m-%d %H:%M:%S}'.format(datetime.datetime.now()))\n",
    "       \n",
    "        url= url_template.format(city,start)   # Instantiate URL\n",
    "        html=requests.get(url)                 # get HTML\n",
    "        b=BeautifulSoup(html.text)             # pass through Beautiful Soup\n",
    "        \n",
    "# Work through the job listings one at a time\n",
    "        results = b.find_all('div',attrs={'class':' row result'})\n",
    "\n",
    "# Extract the individual fields from each result using the functions written\n",
    "        for r in results:\n",
    "            titles.append(extract_title_from_result(r))\n",
    "            companies.append(extract_company_from_result(r))\n",
    "            locations.append(extract_location_from_result(r))\n",
    "            salaries.append(extract_salary_from_result(r))\n",
    "            descriptions.append(extract_description_from_result(r))   # in case do Bonus\n",
    "\n",
    "# Once have all the data, zip the fields together into 'listings'\n",
    "listings_list = zip(titles, companies,locations,salaries,descriptions)\n",
    "\n",
    "# Put the listings into a data frame\n",
    "column_names = ['Title','Company','Location','Salary','Description']   # Column names\n",
    "listings= pd.DataFrame(listings_list,columns=column_names)          # Put into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write to csv file so don't lose\n",
    "file_name='Data Science Jobs 2000.csv'\n",
    "listings.to_csv(file_name,encoding='utf-8',index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "ff98ce64-78a7-441f-a675-63464e32c834"
   },
   "source": [
    "### Clean up the data\n",
    "\n",
    "1. Only a small number of the scraped results have salary information - only these will be used for modeling.\n",
    "1. Some of the salaries are not yearly but hourly or weekly, remove them\n",
    "1. Some of the entries may be duplicated\n",
    "1. The salaries are given as text and usually with ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "focus": false,
    "id": "58533e57-f86b-494a-b841-e7b59c6229c6"
   },
   "outputs": [],
   "source": [
    "# Create a new data frame with rows where salaries contain the phrase 'a year'\n",
    "# This removes all the rows with salary = None or per month or per hour salaries\n",
    "listings2 = listings[listings.Salary.str.contains('a year')]\n",
    "\n",
    "# Remove duplicates\n",
    "listings2.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function convert_salary removes non-numerics, trims off spaces, and convert strings to integers\n",
    "def convert_salary(salary_str):\n",
    "    salary_str = re.sub('[^0-9]','',salary_str).strip()       # strip '$' and ',' and remove spaces\n",
    "    return int(salary_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "focus": false,
    "id": "a0f701e0-80bd-40ba-9101-4535860c0968"
   },
   "outputs": [],
   "source": [
    "# Function clean_salary takes in salary string, removes superfluous characters,\n",
    "# averages a range (if required), and converts to integer\n",
    "def clean_salary(salary_txt):\n",
    "\n",
    "# Remove the words ' a year'\n",
    "    salary_txt= re.sub('a year', '', salary_txt).strip()   # strip 'a year' and trailing blanks\n",
    "\n",
    "# Check if contains a range (has a '-').  If so, divide in two parts and take average\n",
    "    if '-' in salary_txt:\n",
    "        position = salary_txt.find('-')\n",
    "        salary1 = salary_txt[0:position-1]   # before the '-'\n",
    "        salary2 = salary_txt[position+1:]    # after the '-'\n",
    "        salary = (convert_salary(salary1) + convert_salary(salary2))/2   #Convert each to integer, take average       \n",
    "# No salary range so just clean up the salary field\n",
    "    else:\n",
    "        salary = (convert_salary(salary_txt))  # clean up and convert to integer\n",
    "   \n",
    "    return salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run clean_salary on salary column in data frame to get single integer value\n",
    "listings2['Salary']=listings2['Salary'].apply(clean_salary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "43e71edd-210e-42b1-9336-70a931f048af"
   },
   "source": [
    "#### Save results as a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "focus": false,
    "id": "783fd153-28ac-47ab-bfca-27e7c1de95b4"
   },
   "outputs": [],
   "source": [
    "# Export to csv\n",
    "file_name='Data Science Jobs 2000clean.csv'\n",
    "listings2.to_csv(file_name,encoding='utf-8',index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "04563b69-f7b6-466f-9d65-fc62c9ddee6a"
   },
   "source": [
    "## Predict salaries using Random Forests and Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "243e949e-2742-40af-872e-fec475fd306c"
   },
   "source": [
    "#### Load in the the data of scraped salaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "focus": false,
    "id": "588f9845-6143-4bcc-bfd1-85d45b79303d"
   },
   "outputs": [],
   "source": [
    "# Read in the dataframe of cleaned up listings written to csv above\n",
    "listings2=pd.read_csv(file_name,header=0,index_col=0)\n",
    "\n",
    "# Reset the index values\n",
    "listings2.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "c7631f51-07f2-4c79-a093-3e9bc7849a48"
   },
   "source": [
    "#### Predicting a binary variable - whether the salary was low or high. \n",
    "Compute the median salary and create a new binary variable that is true when the salary is high (above the median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "focus": false,
    "id": "c20d2498-151c-44c3-a453-3a333c79a0ac"
   },
   "outputs": [],
   "source": [
    "# compute the median of the salary column\n",
    "median_sal= np.median(listings2.Salary)\n",
    "print 'Median Salary is '+str(median_sal)\n",
    "\n",
    "# Add column to the dataframe that equals 1 if salary is above average and 0 otherwise\n",
    "\n",
    "# Function that determines if salary is above average for the row\n",
    "def above_avg(row):\n",
    "    if row.Salary > median_sal:\n",
    "        above = 1\n",
    "    else:\n",
    "        above = 0\n",
    "    return above\n",
    "\n",
    "# Apply the above_avg function to the listings2 dataframe\n",
    "listings2['High_salary']= listings2.apply(above_avg,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Add columns to the dataframe for City, State, and Metro area since the location field sometimes contains extraneous information like zip code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "focus": false,
    "id": "ddbc6159-6854-4ca7-857f-bfecdaf6d9c2"
   },
   "outputs": [],
   "source": [
    "# City is the parsed out city name.   State is the parsed out state name.\n",
    "# Metro area is the City field that I manually inspect and modify where needed in the csv file,\n",
    "# for example, changing Brooklyn to New York.\n",
    "\n",
    "# Functions that parse the city and state from the location\n",
    "def parse_city(row):\n",
    "    position = row.Location.find(',')\n",
    "    city = row.Location[0:position]   # before the ','\n",
    "    return city\n",
    "\n",
    "def parse_state(row):\n",
    "    position = row.Location.find(',')\n",
    "    state = row.Location[position+2:position+4]   # after the ',' and space\n",
    "    return state\n",
    "\n",
    "# Create City column\n",
    "listings2['City']= listings2.apply(parse_city,axis=1)\n",
    "\n",
    "# Create a Metro column based on City.   I'll manually change this in the CSV to the \n",
    "# surrounding metro area.  \n",
    "listings2['Metro']= listings2.apply(parse_city,axis=1)\n",
    "\n",
    "# Create State column\n",
    "listings2['State']= listings2.apply(parse_state,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "0ef04f32-419c-4bf2-baf7-48201f03df89"
   },
   "source": [
    "#### Create a additional variables for interesting features of a job title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add columns to the database for words 'Senior', 'Manager', 'Analyst', 'Engineer', and phrase 'Data Scientist'\n",
    "\n",
    "# Function that finds string in job title\n",
    "def parse_title(row,string):\n",
    "    if string in row.Title:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Add column for whether 'Senior' is in title\n",
    "word = 'Senior'\n",
    "listings2['Senior']= listings2.apply(parse_title,args=(word,),axis=1)\n",
    "\n",
    "# Add column for whether 'Manager' is in title\n",
    "word = 'Manager'\n",
    "listings2['Manager']= listings2.apply(parse_title,args=(word,),axis=1)\n",
    "\n",
    "# Add column for whether 'Data Scientist' is in title\n",
    "word = 'Data Scientist'\n",
    "listings2['Data_Scientist']= listings2.apply(parse_title,args=(word,),axis=1)\n",
    "\n",
    "# Add column for whether 'Assistant' is in title\n",
    "word = 'Assistant'\n",
    "listings2['Assistant']= listings2.apply(parse_title,args=(word,),axis=1)\n",
    "\n",
    "# Add column for whether 'Analyst' is in title\n",
    "word = 'Analyst'\n",
    "listings2['Analyst']= listings2.apply(parse_title,args=(word,),axis=1)\n",
    "\n",
    "# Add column for whether 'Engineer' is in title\n",
    "word = 'Engineer'\n",
    "listings2['Engineer']= listings2.apply(parse_title,args=(word,),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Export results to csv\n",
    "file_name='Data Science Jobs 2000addcol.csv'\n",
    "listings2.to_csv(file_name,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "4fb29de2-5b98-474c-a4ad-5170b72b9aea"
   },
   "source": [
    "#### Create a Random Forest model to predict High/Low salary using Sklearn. Start by ONLY using the location as a feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in the dataframe of cleaned up listings written to csv above\n",
    "file_name='Data Science Jobs 2000addcol.csv'\n",
    "listings2=pd.read_csv(file_name,header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create random forest model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "rfc = RandomForestClassifier(verbose=1)       #initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "# Build random forests to predict whether high salary based on Metro\n",
    "X = pd.get_dummies(listings2.Metro)   # encode binary variables for Metro\n",
    "y=listings2.High_salary               # y is a series of integers\n",
    "\n",
    "model_rf= rfc.fit(X,y)                # fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.25423729  0.49152542  0.52542373  0.48275862  0.56140351]\n",
      "Random Forest Score:\t0.463 ± 0.108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "# Do cross validation and summarize the scores\n",
    "from sklearn.model_selection import cross_val_score\n",
    "rfscores=cross_val_score(model_rf,X,y,cv=5)\n",
    "print rfscores\n",
    "print \"{} Score:\\t{:0.3} ± {:0.3}\".format(\"Random Forest\", rfscores.mean().round(3), rfscores.std().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Bias isn't much above 50%. (NOTE - varies a lot when rerun.)  Variance is fair.  \n",
    "Metro area by itself is not a good model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "# Build a new Random Forest with additional features. \n",
    "Xrf2 = pd.get_dummies(listings2.drop(['Title','Company','Location','Salary','Description','High_salary','City','State'], axis=1))\n",
    "y=listings2.High_salary              # y is same as above - a series of integers\n",
    "\n",
    "model_rf2= rfc.fit(Xrf2,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.77966102  0.79661017  0.81355932  0.72413793  0.71929825]\n",
      "Random Forest Score:\t0.767 ± 0.038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "# Do cross validation and summarize the scores\n",
    "rf2scores=cross_val_score(model_rf2,Xrf2,y,cv=5)\n",
    "print rf2scores\n",
    "print \"{} Score:\\t{:0.3} ± {:0.3}\".format(\"Random Forest\", rf2scores.mean().round(3), rf2scores.std().round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Vs. just using location (Metro), accuracy has gone up from 58% to 74%, a big improvement\n",
    "Variance has gone down also     #NOTE - actual results changing run to run\n",
    "This looks like a reasonable model, but would like better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Data_Scientist</th>\n",
       "      <td>0.313803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Engineer</th>\n",
       "      <td>0.107850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Analyst</th>\n",
       "      <td>0.085949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metro_San Francisco</th>\n",
       "      <td>0.055675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metro_Philadelphia</th>\n",
       "      <td>0.054414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metro_Chicago</th>\n",
       "      <td>0.051483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Senior</th>\n",
       "      <td>0.049022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metro_Atlanta</th>\n",
       "      <td>0.040860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metro_New York</th>\n",
       "      <td>0.037115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metro_Pittsburgh</th>\n",
       "      <td>0.029333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Manager</th>\n",
       "      <td>0.029055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metro_Seattle</th>\n",
       "      <td>0.024413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metro_Houston</th>\n",
       "      <td>0.021435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metro_Portland</th>\n",
       "      <td>0.018464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metro_Miami</th>\n",
       "      <td>0.014409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metro_Austin</th>\n",
       "      <td>0.012953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metro_Dallas</th>\n",
       "      <td>0.012411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Assistant</th>\n",
       "      <td>0.011865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metro_Los Angeles</th>\n",
       "      <td>0.009731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metro_Phoenix</th>\n",
       "      <td>0.007030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metro_Raleigh</th>\n",
       "      <td>0.006496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metro_Denver</th>\n",
       "      <td>0.006233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     importance\n",
       "Data_Scientist         0.313803\n",
       "Engineer               0.107850\n",
       "Analyst                0.085949\n",
       "Metro_San Francisco    0.055675\n",
       "Metro_Philadelphia     0.054414\n",
       "Metro_Chicago          0.051483\n",
       "Senior                 0.049022\n",
       "Metro_Atlanta          0.040860\n",
       "Metro_New York         0.037115\n",
       "Metro_Pittsburgh       0.029333\n",
       "Manager                0.029055\n",
       "Metro_Seattle          0.024413\n",
       "Metro_Houston          0.021435\n",
       "Metro_Portland         0.018464\n",
       "Metro_Miami            0.014409\n",
       "Metro_Austin           0.012953\n",
       "Metro_Dallas           0.012411\n",
       "Assistant              0.011865\n",
       "Metro_Los Angeles      0.009731\n",
       "Metro_Phoenix          0.007030\n",
       "Metro_Raleigh          0.006496\n",
       "Metro_Denver           0.006233"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the resulting feature importance to see which have the most impact\n",
    "feature_importances = pd.DataFrame(rfc.feature_importances_,\n",
    "                                   index = Xrf2.columns,\n",
    "                                    columns=['importance']).sort_values('importance',\n",
    "                                                                        ascending=False)\n",
    "feature_importances.head(50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This just indicates which words have the most influence, not if it is + or - influence\n",
    "I'll determine later which appear to have + vs. - influence using logisitic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'Title', u'Company', u'Location', u'Salary', u'Description',\n",
       "       u'High_salary', u'City', u'Metro', u'State', u'Senior', u'Manager',\n",
       "       u'Data_Scientist', u'Assistant', u'Analyst', u'Engineer'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listings2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'14564',\n",
       " u'17071701',\n",
       " u'287881',\n",
       " u'500',\n",
       " u'academic',\n",
       " u'access',\n",
       " u'account',\n",
       " u'accountant',\n",
       " u'accounting',\n",
       " u'admin',\n",
       " u'administration',\n",
       " u'administrative',\n",
       " u'advanced',\n",
       " u'advancement',\n",
       " u'advisor',\n",
       " u'ai',\n",
       " u'aids',\n",
       " u'air',\n",
       " u'algorithm',\n",
       " u'alumni',\n",
       " u'anal',\n",
       " u'analysis',\n",
       " u'analyst',\n",
       " u'analytical',\n",
       " u'analytics',\n",
       " u'animal',\n",
       " u'ar',\n",
       " u'architect',\n",
       " u'asc',\n",
       " u'assessment',\n",
       " u'assistant',\n",
       " u'assoc',\n",
       " u'associate',\n",
       " u'asst',\n",
       " u'assurance',\n",
       " u'audit',\n",
       " u'aws',\n",
       " u'azure',\n",
       " u'backend',\n",
       " u'banking',\n",
       " u'bat',\n",
       " u'big',\n",
       " u'billion',\n",
       " u'bio',\n",
       " u'bioinformatics',\n",
       " u'biologics',\n",
       " u'biology',\n",
       " u'biosolids',\n",
       " u'biostatistics',\n",
       " u'buildout',\n",
       " u'bureau',\n",
       " u'business',\n",
       " u'capital',\n",
       " u'cards',\n",
       " u'caretaker',\n",
       " u'cata',\n",
       " u'cellular',\n",
       " u'center',\n",
       " u'certifying',\n",
       " u'chain',\n",
       " u'chair',\n",
       " u'chemist',\n",
       " u'chemistry',\n",
       " u'chief',\n",
       " u'child',\n",
       " u'children',\n",
       " u'civil',\n",
       " u'client',\n",
       " u'clinical',\n",
       " u'cloning',\n",
       " u'code',\n",
       " u'coding',\n",
       " u'commiss',\n",
       " u'commissioner',\n",
       " u'company',\n",
       " u'compliance',\n",
       " u'computational',\n",
       " u'computer',\n",
       " u'computing',\n",
       " u'consort',\n",
       " u'consultancy',\n",
       " u'consulting',\n",
       " u'contr',\n",
       " u'contro',\n",
       " u'control',\n",
       " u'coordinator',\n",
       " u'core',\n",
       " u'crail',\n",
       " u'credit',\n",
       " u'criminal',\n",
       " u'cybersecurity',\n",
       " u'd3',\n",
       " u'data',\n",
       " u'democracy',\n",
       " u'develop',\n",
       " u'developer',\n",
       " u'development',\n",
       " u'devops',\n",
       " u'dir',\n",
       " u'director',\n",
       " u'discovery',\n",
       " u'disease',\n",
       " u'district',\n",
       " u'division',\n",
       " u'doctoral',\n",
       " u'dollar',\n",
       " u'drug',\n",
       " u'earth',\n",
       " u'editor',\n",
       " u'elisa',\n",
       " u'emergency',\n",
       " u'end',\n",
       " u'energy',\n",
       " u'engineer',\n",
       " u'engineering',\n",
       " u'engineers',\n",
       " u'enterprise',\n",
       " u'entrepreneurship',\n",
       " u'environmental',\n",
       " u'epidemiology',\n",
       " u'etl',\n",
       " u'evaluation',\n",
       " u'experimental',\n",
       " u'facing',\n",
       " u'famil',\n",
       " u'family',\n",
       " u'fellow',\n",
       " u'field',\n",
       " u'file',\n",
       " u'filled',\n",
       " u'financial',\n",
       " u'fortune',\n",
       " u'frontend',\n",
       " u'gene',\n",
       " u'generation',\n",
       " u'genetics',\n",
       " u'geospatial',\n",
       " u'global',\n",
       " u'government',\n",
       " u'gr0218',\n",
       " u'grade',\n",
       " u'group',\n",
       " u'hadoop',\n",
       " u'hazard',\n",
       " u'head',\n",
       " u'healt',\n",
       " u'health',\n",
       " u'healthcare',\n",
       " u'hedgefund',\n",
       " u'hiv',\n",
       " u'hypertension',\n",
       " u'ibs',\n",
       " u'id',\n",
       " u'ii',\n",
       " u'iii',\n",
       " u'immunohematology',\n",
       " u'immunology',\n",
       " u'industry',\n",
       " u'infant',\n",
       " u'informatics',\n",
       " u'information',\n",
       " u'initiative',\n",
       " u'institute',\n",
       " u'institutional',\n",
       " u'instructor',\n",
       " u'integration',\n",
       " u'investigations',\n",
       " u'investigator',\n",
       " u'ios',\n",
       " u'ir',\n",
       " u'java',\n",
       " u'jerry',\n",
       " u'job',\n",
       " u'johnson',\n",
       " u'justice',\n",
       " u'lab',\n",
       " u'laboratory',\n",
       " u'lc',\n",
       " u'lead',\n",
       " u'leader',\n",
       " u'learning',\n",
       " u'lecturer',\n",
       " u'legal',\n",
       " u'level',\n",
       " u'librarian',\n",
       " u'library',\n",
       " u'machine',\n",
       " u'maintenance',\n",
       " u'management',\n",
       " u'manager',\n",
       " u'map',\n",
       " u'mapping',\n",
       " u'marketing',\n",
       " u'master',\n",
       " u'maternal',\n",
       " u'med',\n",
       " u'medical',\n",
       " u'medicinal',\n",
       " u'mental',\n",
       " u'method',\n",
       " u'microstrategy',\n",
       " u'mid',\n",
       " u'ml',\n",
       " u'mobile',\n",
       " u'model',\n",
       " u'modeler',\n",
       " u'modeling',\n",
       " u'molecular',\n",
       " u'ms',\n",
       " u'multi',\n",
       " u'multivariate',\n",
       " u'mysql',\n",
       " u'natural',\n",
       " u'nature',\n",
       " u'navigator',\n",
       " u'networks',\n",
       " u'neural',\n",
       " u'neuro',\n",
       " u'neurodegeneration',\n",
       " u'nlp',\n",
       " u'nmr',\n",
       " u'octri',\n",
       " u'office',\n",
       " u'oncology',\n",
       " u'open',\n",
       " u'operations',\n",
       " u'optimization',\n",
       " u'ospe',\n",
       " u'outreach',\n",
       " u'overdose',\n",
       " u'panel',\n",
       " u'parks',\n",
       " u'pca',\n",
       " u'peco',\n",
       " u'pest',\n",
       " u'pharma',\n",
       " u'pharmacology',\n",
       " u'phd',\n",
       " u'physical',\n",
       " u'planner',\n",
       " u'platform',\n",
       " u'policing',\n",
       " u'policy',\n",
       " u'posit',\n",
       " u'post',\n",
       " u'postdoctoral',\n",
       " u'predictive',\n",
       " u'prevention',\n",
       " u'primary',\n",
       " u'principal',\n",
       " u'production',\n",
       " u'program',\n",
       " u'programmer',\n",
       " u'project',\n",
       " u'projects',\n",
       " u'prospect',\n",
       " u'protein',\n",
       " u'provider',\n",
       " u'psychiatry',\n",
       " u'publi',\n",
       " u'public',\n",
       " u'python',\n",
       " u'quality',\n",
       " u'quant',\n",
       " u'quantitative',\n",
       " u'radiation',\n",
       " u'ranking',\n",
       " u'redshift',\n",
       " u'registr',\n",
       " u'relations',\n",
       " u'repro',\n",
       " u'req',\n",
       " u'research',\n",
       " u'researcher',\n",
       " u'risk',\n",
       " u'rshiny',\n",
       " u'ruby',\n",
       " u'sales',\n",
       " u'sas',\n",
       " u'scholar',\n",
       " u'science',\n",
       " u'sciences',\n",
       " u'scientist',\n",
       " u'scientists',\n",
       " u'se',\n",
       " u'search',\n",
       " u'sections',\n",
       " u'security',\n",
       " u'senior',\n",
       " u'sequencing',\n",
       " u'service',\n",
       " u'services',\n",
       " u'site',\n",
       " u'social',\n",
       " u'software',\n",
       " u'som',\n",
       " u'spark',\n",
       " u'special',\n",
       " u'specialist',\n",
       " u'spectroscopist',\n",
       " u'sponsored',\n",
       " u'spss',\n",
       " u'sql',\n",
       " u'sr',\n",
       " u'ssie',\n",
       " u'stack',\n",
       " u'startup',\n",
       " u'statistical',\n",
       " u'statistician',\n",
       " u'statistics',\n",
       " u'strengthening',\n",
       " u'study',\n",
       " u'success',\n",
       " u'sumh',\n",
       " u'supply',\n",
       " u'support',\n",
       " u'sutardja',\n",
       " u'systems',\n",
       " u'tableau',\n",
       " u'team',\n",
       " u'technical',\n",
       " u'technician',\n",
       " u'technologist',\n",
       " u'technology',\n",
       " u'temporary',\n",
       " u'therapy',\n",
       " u'thoracic',\n",
       " u'tier',\n",
       " u'time',\n",
       " u'tmf',\n",
       " u'toxicology',\n",
       " u'trade',\n",
       " u'translational',\n",
       " u'trial',\n",
       " u'validation',\n",
       " u'variant',\n",
       " u'varies',\n",
       " u'veterinary',\n",
       " u'vision',\n",
       " u'visionin',\n",
       " u'vivo',\n",
       " u'vp',\n",
       " u'warm',\n",
       " u'wastewater',\n",
       " u'web',\n",
       " u'welfare',\n",
       " u'world',\n",
       " u'writing',\n",
       " u'youth']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use count-vectorizer to create additional features based on the words in the job titles.\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer(analyzer=\"word\",stop_words='english',decode_error='ignore')\n",
    "title_features = cv.fit_transform(listings2.Title)\n",
    "cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Resulting feature names look fairly clean except for few numerics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build a new random forest model with location and these new features included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(292, 349)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move the featues to an array and a dataframe\n",
    "title_features_array = title_features.toarray()\n",
    "title_features_df = pd.DataFrame(title_features_array,columns=cv.get_feature_names())\n",
    "title_features_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(292,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listings2['Metro'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(292, 365)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index([     u'Atlanta',       u'Austin',      u'Chicago',       u'Dallas',\n",
       "             u'Denver',      u'Houston',  u'Los Angeles',        u'Miami',\n",
       "           u'New York', u'Philadelphia',\n",
       "       ...\n",
       "           u'visionin',         u'vivo',           u'vp',         u'warm',\n",
       "         u'wastewater',          u'web',      u'welfare',        u'world',\n",
       "            u'writing',        u'youth'],\n",
       "      dtype='object', length=365)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For the x variable, concatenate Metro (location) with \n",
    "# the new features from the count vectorizer\n",
    "Xrf3 = pd.concat([pd.get_dummies(listings2.Metro),title_features_df],axis=1)\n",
    "print Xrf3.shape\n",
    "Xrf3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "# Fit the 3rd random forest model to predict High Salary using Metro (location) and all the words in the titles\n",
    "y=listings2.High_salary              # y is same as above - a series of integers\n",
    "model_rf3= rfc.fit(Xrf3,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.77966102  0.76271186  0.83050847  0.81034483  0.73684211]\n",
      "Random Forest Score:\t0.784 ± 0.033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "# Do cross validation and summarize the scores\n",
    "from sklearn.model_selection import cross_val_score\n",
    "rf3scores=cross_val_score(model_rf3,Xrf3,y,cv=5)\n",
    "print rf3scores\n",
    "print \"{} Score:\\t{:0.3} ± {:0.3}\".format(\"Random Forest\", rf3scores.mean().round(3), rf3scores.std().round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Prediction accuracy improved some with addition of all words in the title, but variance went up slightly \n",
    "\n",
    "To summarize (NOTE - values vary slightly from run to run):\n",
    "With just Metro (Location): Random Forest Score:\t0.585 ± 0.072\n",
    "With the addition of words I chose: Random Forest Score:\t0.739 ± 0.037\n",
    "With the addition of all the words in the title: Random Forest Score:\t0.804 ± 0.044 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>data</th>\n",
       "      <td>0.071868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scientist</th>\n",
       "      <td>0.063806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>research</th>\n",
       "      <td>0.038561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analyst</th>\n",
       "      <td>0.037576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning</th>\n",
       "      <td>0.030206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>machine</th>\n",
       "      <td>0.028130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Philadelphia</th>\n",
       "      <td>0.023879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New York</th>\n",
       "      <td>0.020670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lead</th>\n",
       "      <td>0.017583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Atlanta</th>\n",
       "      <td>0.015781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Houston</th>\n",
       "      <td>0.015459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>senior</th>\n",
       "      <td>0.015229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>engineer</th>\n",
       "      <td>0.015024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>director</th>\n",
       "      <td>0.014610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chicago</th>\n",
       "      <td>0.014524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>associate</th>\n",
       "      <td>0.014219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>principal</th>\n",
       "      <td>0.012865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pittsburgh</th>\n",
       "      <td>0.011002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Portland</th>\n",
       "      <td>0.010919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analytics</th>\n",
       "      <td>0.010756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>science</th>\n",
       "      <td>0.010017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>San Francisco</th>\n",
       "      <td>0.009888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marketing</th>\n",
       "      <td>0.009665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stack</th>\n",
       "      <td>0.009249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>risk</th>\n",
       "      <td>0.009192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>geospatial</th>\n",
       "      <td>0.008991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quantitative</th>\n",
       "      <td>0.008389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>laboratory</th>\n",
       "      <td>0.008247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bureau</th>\n",
       "      <td>0.008213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sr</th>\n",
       "      <td>0.008211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>informatics</th>\n",
       "      <td>0.008017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>statistical</th>\n",
       "      <td>0.007887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manager</th>\n",
       "      <td>0.007739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>district</th>\n",
       "      <td>0.007325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>assistant</th>\n",
       "      <td>0.007297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>software</th>\n",
       "      <td>0.007023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clinical</th>\n",
       "      <td>0.006813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Raleigh</th>\n",
       "      <td>0.006655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>architect</th>\n",
       "      <td>0.006604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iii</th>\n",
       "      <td>0.006352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Austin</th>\n",
       "      <td>0.006097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>developer</th>\n",
       "      <td>0.006049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>head</th>\n",
       "      <td>0.006023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ai</th>\n",
       "      <td>0.005960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>big</th>\n",
       "      <td>0.005920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phoenix</th>\n",
       "      <td>0.005894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation</th>\n",
       "      <td>0.005872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>biology</th>\n",
       "      <td>0.005796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accountant</th>\n",
       "      <td>0.005633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scientists</th>\n",
       "      <td>0.005547</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               importance\n",
       "data             0.071868\n",
       "scientist        0.063806\n",
       "research         0.038561\n",
       "analyst          0.037576\n",
       "learning         0.030206\n",
       "machine          0.028130\n",
       "Philadelphia     0.023879\n",
       "New York         0.020670\n",
       "lead             0.017583\n",
       "Atlanta          0.015781\n",
       "Houston          0.015459\n",
       "senior           0.015229\n",
       "engineer         0.015024\n",
       "director         0.014610\n",
       "Chicago          0.014524\n",
       "associate        0.014219\n",
       "principal        0.012865\n",
       "Pittsburgh       0.011002\n",
       "Portland         0.010919\n",
       "analytics        0.010756\n",
       "science          0.010017\n",
       "San Francisco    0.009888\n",
       "marketing        0.009665\n",
       "stack            0.009249\n",
       "risk             0.009192\n",
       "geospatial       0.008991\n",
       "quantitative     0.008389\n",
       "laboratory       0.008247\n",
       "bureau           0.008213\n",
       "sr               0.008211\n",
       "informatics      0.008017\n",
       "statistical      0.007887\n",
       "manager          0.007739\n",
       "district         0.007325\n",
       "assistant        0.007297\n",
       "software         0.007023\n",
       "clinical         0.006813\n",
       "Raleigh          0.006655\n",
       "architect        0.006604\n",
       "iii              0.006352\n",
       "Austin           0.006097\n",
       "developer        0.006049\n",
       "head             0.006023\n",
       "ai               0.005960\n",
       "big              0.005920\n",
       "Phoenix          0.005894\n",
       "validation       0.005872\n",
       "biology          0.005796\n",
       "accountant       0.005633\n",
       "scientists       0.005547"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at which words in the title and which locations had the greatest feature importance\n",
    "feature_importances = pd.DataFrame(rfc.feature_importances_,\n",
    "                                   index = Xrf3.columns,\n",
    "                                    columns=['importance']).sort_values('importance',\n",
    "                                                                        ascending=False)\n",
    "feature_importances.head(50)\n",
    "# This just indicates which words have the most influence, not if it is + or - influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location Importances (highest to lowest - varies from run to run)\n",
      "Philadelphia: 0.0238793433402\n",
      "New York: 0.020670307324\n",
      "San Francisco: 0.009887976573\n",
      "Atlanta: 0.015781381455\n",
      "Chicago: 0.0145235067191\n",
      "Pittsburgh: 0.0110023142789\n",
      "Miami: 0.00452748235648\n",
      "Phoenix: 0.00589379783303\n",
      "Austin: 0.00609678880759\n",
      "Portland: 0.0109188165585\n",
      "Houston: 0.0154586974128\n",
      "Denver: 0.00197320731529\n",
      "Seattle: 0.00385032467514\n",
      "Los Angeles: 0.00423201595799\n",
      "Dallas: 0.00180150226465\n",
      "Raleigh: 0.00665545742795\n"
     ]
    }
   ],
   "source": [
    "# For reference, print the location importance\n",
    "print 'Location Importances (highest to lowest - varies from run to run)'\n",
    "print 'Philadelphia: '+str(feature_importances.get_value('Philadelphia','importance'))\n",
    "print 'New York: '+str(feature_importances.get_value('New York','importance'))\n",
    "print 'San Francisco: '+str(feature_importances.get_value('San Francisco','importance'))\n",
    "print 'Atlanta: '+str(feature_importances.get_value('Atlanta','importance'))\n",
    "print 'Chicago: '+str(feature_importances.get_value('Chicago','importance'))\n",
    "print 'Pittsburgh: '+str(feature_importances.get_value('Pittsburgh','importance'))\n",
    "print 'Miami: '+str(feature_importances.get_value('Miami','importance'))\n",
    "print 'Phoenix: '+str(feature_importances.get_value('Phoenix','importance'))\n",
    "print 'Austin: '+str(feature_importances.get_value('Austin','importance'))\n",
    "print 'Portland: '+str(feature_importances.get_value('Portland','importance'))\n",
    "print 'Houston: '+str(feature_importances.get_value('Houston','importance'))\n",
    "print 'Denver: '+str(feature_importances.get_value('Denver','importance'))\n",
    "print 'Seattle: '+str(feature_importances.get_value('Seattle','importance'))\n",
    "print 'Los Angeles: '+str(feature_importances.get_value('Los Angeles','importance'))\n",
    "print 'Dallas: '+str(feature_importances.get_value('Dallas','importance'))\n",
    "print 'Raleigh: '+str(feature_importances.get_value('Raleigh','importance'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ones at the low end of the ranking have little impact so assumably are near the median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repeat the model-building process using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Do logistic regression for Metro (location) only.   Reuse X and y from above\n",
    "lr = LogisticRegression()       #initialize\n",
    "model_lr= lr.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.57627119  0.49152542  0.52542373  0.48275862  0.50877193]\n",
      "Logistic Regression Score:\t0.517 ± 0.033\n"
     ]
    }
   ],
   "source": [
    "# Do cross validation and summarize the scores\n",
    "lrscores=cross_val_score(model_lr,X,y,cv=5)\n",
    "print lrscores\n",
    "print \"{} Score:\\t{:0.3} ± {:0.3}\".format(\"Logistic Regression\", lrscores.mean().round(3), lrscores.std().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This model with just the location has poor prediction accuracy \n",
    "Compare to random forest with same inputs: Random Forest Score:\t0.585 ± 0.072\n",
    "Random forest had better prediction accuracy but higher variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>San Francisco</th>\n",
       "      <td>1.488790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Philadelphia</th>\n",
       "      <td>1.265302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chicago</th>\n",
       "      <td>0.984287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Austin</th>\n",
       "      <td>0.922431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Raleigh</th>\n",
       "      <td>0.583647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Seattle</th>\n",
       "      <td>0.451859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Los Angeles</th>\n",
       "      <td>0.239937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New York</th>\n",
       "      <td>-0.118353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Atlanta</th>\n",
       "      <td>-0.166023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Denver</th>\n",
       "      <td>-0.222972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dallas</th>\n",
       "      <td>-0.449089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Houston</th>\n",
       "      <td>-0.894762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Miami</th>\n",
       "      <td>-0.956977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phoenix</th>\n",
       "      <td>-0.956977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Portland</th>\n",
       "      <td>-1.134025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pittsburgh</th>\n",
       "      <td>-1.207624</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                weights\n",
       "San Francisco  1.488790\n",
       "Philadelphia   1.265302\n",
       "Chicago        0.984287\n",
       "Austin         0.922431\n",
       "Raleigh        0.583647\n",
       "Seattle        0.451859\n",
       "Los Angeles    0.239937\n",
       "New York      -0.118353\n",
       "Atlanta       -0.166023\n",
       "Denver        -0.222972\n",
       "Dallas        -0.449089\n",
       "Houston       -0.894762\n",
       "Miami         -0.956977\n",
       "Phoenix       -0.956977\n",
       "Portland      -1.134025\n",
       "Pittsburgh    -1.207624"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put coefficients into readable format\n",
    "coeff_vals = pd.DataFrame(lr.coef_.transpose(),index = X.columns,\n",
    "                                   columns=['weights']).sort_values('weights',\n",
    "                                                                       ascending=False)\n",
    "coeff_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Jobs in San Francisco, Philadelphia, and Chicago pay more than jobs in Pittsburgh, Portland,\n",
    "Phoenix, and Miami as examples.   Others are in between"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.81355932  0.79661017  0.79661017  0.77586207  0.71929825]\n",
      "Logistic Regression Score:\t0.78 ± 0.033\n"
     ]
    }
   ],
   "source": [
    "# Do logistic regression including the title keywords that I identified\n",
    "# Reuse Xrf2 and y from above\n",
    "lr2 = LogisticRegression()       #initialize\n",
    "model_lr2= lr2.fit(Xrf2,y)\n",
    "\n",
    "# Do cross validation and summarize the scores\n",
    "lr2scores=cross_val_score(model_lr2,Xrf2,y,cv=5)\n",
    "print lr2scores\n",
    "print \"{} Score:\\t{:0.3} ± {:0.3}\".format(\"Logistic Regression\", lr2scores.mean().round(3), lr2scores.std().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For linear regression, prediction accuracy has increased with similar variance\n",
    "Comparing to the random forest model with the same inputs: Random Forest Score:\t0.739 ± 0.037\n",
    "This is slightly more accurate than the random forest model with similar variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weights</th>\n",
       "      <th>Likelihood_Change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Data_Scientist</th>\n",
       "      <td>2.420162</td>\n",
       "      <td>11.247680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metro_San Francisco</th>\n",
       "      <td>1.332194</td>\n",
       "      <td>3.789349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Engineer</th>\n",
       "      <td>1.202103</td>\n",
       "      <td>3.327107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metro_Philadelphia</th>\n",
       "      <td>1.029181</td>\n",
       "      <td>2.798773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metro_Austin</th>\n",
       "      <td>0.586749</td>\n",
       "      <td>1.798133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metro_Seattle</th>\n",
       "      <td>0.505119</td>\n",
       "      <td>1.657183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Senior</th>\n",
       "      <td>0.467914</td>\n",
       "      <td>1.596660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metro_Chicago</th>\n",
       "      <td>0.357732</td>\n",
       "      <td>1.430083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Manager</th>\n",
       "      <td>0.278444</td>\n",
       "      <td>1.321073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metro_Los Angeles</th>\n",
       "      <td>0.145275</td>\n",
       "      <td>1.156358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metro_Raleigh</th>\n",
       "      <td>0.005542</td>\n",
       "      <td>1.005557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metro_Dallas</th>\n",
       "      <td>-0.058845</td>\n",
       "      <td>0.942853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metro_Denver</th>\n",
       "      <td>-0.108579</td>\n",
       "      <td>0.897108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metro_New York</th>\n",
       "      <td>-0.369154</td>\n",
       "      <td>0.691319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metro_Atlanta</th>\n",
       "      <td>-0.377615</td>\n",
       "      <td>0.685495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metro_Houston</th>\n",
       "      <td>-0.403384</td>\n",
       "      <td>0.668056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Analyst</th>\n",
       "      <td>-0.476264</td>\n",
       "      <td>0.621100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metro_Phoenix</th>\n",
       "      <td>-0.592157</td>\n",
       "      <td>0.553133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metro_Miami</th>\n",
       "      <td>-0.713100</td>\n",
       "      <td>0.490123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metro_Portland</th>\n",
       "      <td>-0.753315</td>\n",
       "      <td>0.470803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Assistant</th>\n",
       "      <td>-0.847224</td>\n",
       "      <td>0.428603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metro_Pittsburgh</th>\n",
       "      <td>-1.279543</td>\n",
       "      <td>0.278164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      weights  Likelihood_Change\n",
       "Data_Scientist       2.420162          11.247680\n",
       "Metro_San Francisco  1.332194           3.789349\n",
       "Engineer             1.202103           3.327107\n",
       "Metro_Philadelphia   1.029181           2.798773\n",
       "Metro_Austin         0.586749           1.798133\n",
       "Metro_Seattle        0.505119           1.657183\n",
       "Senior               0.467914           1.596660\n",
       "Metro_Chicago        0.357732           1.430083\n",
       "Manager              0.278444           1.321073\n",
       "Metro_Los Angeles    0.145275           1.156358\n",
       "Metro_Raleigh        0.005542           1.005557\n",
       "Metro_Dallas        -0.058845           0.942853\n",
       "Metro_Denver        -0.108579           0.897108\n",
       "Metro_New York      -0.369154           0.691319\n",
       "Metro_Atlanta       -0.377615           0.685495\n",
       "Metro_Houston       -0.403384           0.668056\n",
       "Analyst             -0.476264           0.621100\n",
       "Metro_Phoenix       -0.592157           0.553133\n",
       "Metro_Miami         -0.713100           0.490123\n",
       "Metro_Portland      -0.753315           0.470803\n",
       "Assistant           -0.847224           0.428603\n",
       "Metro_Pittsburgh    -1.279543           0.278164"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put coefficients into readable format\n",
    "coeff_vals2 = pd.DataFrame(lr2.coef_.transpose(),index = Xrf2.columns,\n",
    "                                   columns=['weights']).sort_values('weights',\n",
    "                                                                       ascending=False)\n",
    "\n",
    "# Raise e to the coefficient column to get the likelihood of being above the median\n",
    "coeff_vals2['Likelihood_Change'] = np.exp(coeff_vals2['weights'])\n",
    "\n",
    "coeff_vals2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Comparing to the random forest, many of the words that it listed as most impactful are \n",
    "showing up with high postive or high negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.79661017  0.76271186  0.81355932  0.81034483  0.73684211]\n",
      "Logisitic Regression Score:\t0.784 ± 0.03\n"
     ]
    }
   ],
   "source": [
    "# Do logistic regression including all the title words identified by the count vectorizer\n",
    "# Reuse Xrf3 and y from above\n",
    "lr3 = LogisticRegression()       #initialize\n",
    "model_lr3= lr3.fit(Xrf3,y)\n",
    "\n",
    "# Do cross validation and summarize the scores\n",
    "lr3scores=cross_val_score(model_lr3,Xrf3,y,cv=5)\n",
    "print lr3scores\n",
    "print \"{} Score:\\t{:0.3} ± {:0.3}\".format(\"Logisitic Regression\", lr3scores.mean().round(3), lr3scores.std().round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For logistic regression, this is practically equivalent in accuracy and variance\n",
    "to the model with the title keywords I identified.   \n",
    "The random forest model performed slightly more accuarately, with higher variance\n",
    "Random Forest Score:\t0.804 ± 0.044 \n",
    "(Note about variance - a rerun of the random forest gave score of 0.784 ± 0.014)\n",
    "So although random forest might be a bit better we can't say so with any certainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weights</th>\n",
       "      <th>Likelihood_Change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>quantitative</th>\n",
       "      <td>1.477360</td>\n",
       "      <td>4.381365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data</th>\n",
       "      <td>1.185176</td>\n",
       "      <td>3.271264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>director</th>\n",
       "      <td>1.166122</td>\n",
       "      <td>3.209521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>machine</th>\n",
       "      <td>1.084979</td>\n",
       "      <td>2.959378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning</th>\n",
       "      <td>1.084979</td>\n",
       "      <td>2.959378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Philadelphia</th>\n",
       "      <td>0.986667</td>\n",
       "      <td>2.682278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>principal</th>\n",
       "      <td>0.981386</td>\n",
       "      <td>2.668150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lead</th>\n",
       "      <td>0.909156</td>\n",
       "      <td>2.482226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scientist</th>\n",
       "      <td>0.868119</td>\n",
       "      <td>2.382425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>informatics</th>\n",
       "      <td>0.852438</td>\n",
       "      <td>2.345358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>risk</th>\n",
       "      <td>0.819035</td>\n",
       "      <td>2.268309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>architect</th>\n",
       "      <td>0.769539</td>\n",
       "      <td>2.158771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>predictive</th>\n",
       "      <td>0.750518</td>\n",
       "      <td>2.118097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sr</th>\n",
       "      <td>0.724300</td>\n",
       "      <td>2.063286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sales</th>\n",
       "      <td>0.711377</td>\n",
       "      <td>2.036793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>science</th>\n",
       "      <td>0.693570</td>\n",
       "      <td>2.000846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quant</th>\n",
       "      <td>0.675500</td>\n",
       "      <td>1.965016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>San Francisco</th>\n",
       "      <td>0.674083</td>\n",
       "      <td>1.962234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>district</th>\n",
       "      <td>0.645465</td>\n",
       "      <td>1.906873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <td>0.625013</td>\n",
       "      <td>1.868270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>head</th>\n",
       "      <td>0.594253</td>\n",
       "      <td>1.811678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>engineering</th>\n",
       "      <td>0.574547</td>\n",
       "      <td>1.776325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Seattle</th>\n",
       "      <td>0.551057</td>\n",
       "      <td>1.735087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ios</th>\n",
       "      <td>0.513069</td>\n",
       "      <td>1.670409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>biology</th>\n",
       "      <td>0.509378</td>\n",
       "      <td>1.664255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>translational</th>\n",
       "      <td>0.509378</td>\n",
       "      <td>1.664255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Austin</th>\n",
       "      <td>0.491146</td>\n",
       "      <td>1.634187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation</th>\n",
       "      <td>0.478496</td>\n",
       "      <td>1.613645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microstrategy</th>\n",
       "      <td>0.456082</td>\n",
       "      <td>1.577880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mapping</th>\n",
       "      <td>0.454989</td>\n",
       "      <td>1.576156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>child</th>\n",
       "      <td>-0.414776</td>\n",
       "      <td>0.660488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oncology</th>\n",
       "      <td>-0.416320</td>\n",
       "      <td>0.659469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>administration</th>\n",
       "      <td>-0.419576</td>\n",
       "      <td>0.657325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>technician</th>\n",
       "      <td>-0.456132</td>\n",
       "      <td>0.633730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accountant</th>\n",
       "      <td>-0.472566</td>\n",
       "      <td>0.623400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>policy</th>\n",
       "      <td>-0.491130</td>\n",
       "      <td>0.611935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coordinator</th>\n",
       "      <td>-0.499729</td>\n",
       "      <td>0.606695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mysql</th>\n",
       "      <td>-0.514665</td>\n",
       "      <td>0.597701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>redshift</th>\n",
       "      <td>-0.514665</td>\n",
       "      <td>0.597701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lab</th>\n",
       "      <td>-0.517893</td>\n",
       "      <td>0.595775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>developer</th>\n",
       "      <td>-0.518028</td>\n",
       "      <td>0.595694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ar</th>\n",
       "      <td>-0.527942</td>\n",
       "      <td>0.589818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Miami</th>\n",
       "      <td>-0.540383</td>\n",
       "      <td>0.582525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iii</th>\n",
       "      <td>-0.554346</td>\n",
       "      <td>0.574448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medical</th>\n",
       "      <td>-0.583624</td>\n",
       "      <td>0.557873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>geospatial</th>\n",
       "      <td>-0.587928</td>\n",
       "      <td>0.555477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phoenix</th>\n",
       "      <td>-0.596819</td>\n",
       "      <td>0.550560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bureau</th>\n",
       "      <td>-0.613462</td>\n",
       "      <td>0.541473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bioinformatics</th>\n",
       "      <td>-0.616921</td>\n",
       "      <td>0.539603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>som</th>\n",
       "      <td>-0.642754</td>\n",
       "      <td>0.525842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clinical</th>\n",
       "      <td>-0.711973</td>\n",
       "      <td>0.490675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>environmental</th>\n",
       "      <td>-0.716530</td>\n",
       "      <td>0.488444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analyst</th>\n",
       "      <td>-0.769416</td>\n",
       "      <td>0.463284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>specialist</th>\n",
       "      <td>-0.783030</td>\n",
       "      <td>0.457019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>associate</th>\n",
       "      <td>-0.788759</td>\n",
       "      <td>0.454408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>assistant</th>\n",
       "      <td>-0.797583</td>\n",
       "      <td>0.450416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>laboratory</th>\n",
       "      <td>-0.827570</td>\n",
       "      <td>0.437110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Portland</th>\n",
       "      <td>-0.833051</td>\n",
       "      <td>0.434721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>research</th>\n",
       "      <td>-0.887263</td>\n",
       "      <td>0.411781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pittsburgh</th>\n",
       "      <td>-1.027219</td>\n",
       "      <td>0.358001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>365 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 weights  Likelihood_Change\n",
       "quantitative    1.477360           4.381365\n",
       "data            1.185176           3.271264\n",
       "director        1.166122           3.209521\n",
       "machine         1.084979           2.959378\n",
       "learning        1.084979           2.959378\n",
       "Philadelphia    0.986667           2.682278\n",
       "principal       0.981386           2.668150\n",
       "lead            0.909156           2.482226\n",
       "scientist       0.868119           2.382425\n",
       "informatics     0.852438           2.345358\n",
       "risk            0.819035           2.268309\n",
       "architect       0.769539           2.158771\n",
       "predictive      0.750518           2.118097\n",
       "sr              0.724300           2.063286\n",
       "sales           0.711377           2.036793\n",
       "science         0.693570           2.000846\n",
       "quant           0.675500           1.965016\n",
       "San Francisco   0.674083           1.962234\n",
       "district        0.645465           1.906873\n",
       "model           0.625013           1.868270\n",
       "head            0.594253           1.811678\n",
       "engineering     0.574547           1.776325\n",
       "Seattle         0.551057           1.735087\n",
       "ios             0.513069           1.670409\n",
       "biology         0.509378           1.664255\n",
       "translational   0.509378           1.664255\n",
       "Austin          0.491146           1.634187\n",
       "validation      0.478496           1.613645\n",
       "microstrategy   0.456082           1.577880\n",
       "mapping         0.454989           1.576156\n",
       "...                  ...                ...\n",
       "child          -0.414776           0.660488\n",
       "oncology       -0.416320           0.659469\n",
       "administration -0.419576           0.657325\n",
       "technician     -0.456132           0.633730\n",
       "accountant     -0.472566           0.623400\n",
       "policy         -0.491130           0.611935\n",
       "coordinator    -0.499729           0.606695\n",
       "mysql          -0.514665           0.597701\n",
       "redshift       -0.514665           0.597701\n",
       "lab            -0.517893           0.595775\n",
       "developer      -0.518028           0.595694\n",
       "ar             -0.527942           0.589818\n",
       "Miami          -0.540383           0.582525\n",
       "iii            -0.554346           0.574448\n",
       "medical        -0.583624           0.557873\n",
       "geospatial     -0.587928           0.555477\n",
       "Phoenix        -0.596819           0.550560\n",
       "bureau         -0.613462           0.541473\n",
       "bioinformatics -0.616921           0.539603\n",
       "som            -0.642754           0.525842\n",
       "clinical       -0.711973           0.490675\n",
       "environmental  -0.716530           0.488444\n",
       "analyst        -0.769416           0.463284\n",
       "specialist     -0.783030           0.457019\n",
       "associate      -0.788759           0.454408\n",
       "assistant      -0.797583           0.450416\n",
       "laboratory     -0.827570           0.437110\n",
       "Portland       -0.833051           0.434721\n",
       "research       -0.887263           0.411781\n",
       "Pittsburgh     -1.027219           0.358001\n",
       "\n",
       "[365 rows x 2 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put coefficients into readable format\n",
    "coeff_vals3 = pd.DataFrame(lr3.coef_.transpose(),index = Xrf3.columns,\n",
    "                                   columns=['weights']).sort_values('weights',\n",
    "                                                                       ascending=False)\n",
    "# Raise e to the coefficient column to get the likelihood of being above the median\n",
    "coeff_vals3['Likelihood_Change'] = np.exp(coeff_vals3['weights'])\n",
    "coeff_vals3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Much of this lines up with words that the random forest showed had the most influence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### With Grid search rerun logistic regression with L1 regularization and range of C (inverse of alpha) to try to determine key variables in Title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LassoCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialize model, setup parameters\n",
    "lrt2 = LogisticRegression(penalty='l1')      # Lasso = l1\n",
    "model_lrt2= lrt2.fit(Xrf3,y)\n",
    "\n",
    "Cs=np.array([0.001,0.01,0.1,1])\n",
    "\n",
    "# fit model\n",
    "gridcv_lrt2=GridSearchCV(estimator=lrt2, param_grid=dict(C=Cs))\n",
    "gridcv_lrt2.fit(Xrf3,y)\n",
    "\n",
    "# grid.cv_results_   \n",
    "gridcv_lrt2.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adjust c range and rerun\n",
    "\n",
    "Cs=np.array([0.5,0.8,1,5,10])\n",
    "\n",
    "# fit model\n",
    "gridcv_lrt2=GridSearchCV(estimator=lrt2, param_grid=dict(C=Cs))\n",
    "gridcv_lrt2.fit(Xrf3,y)\n",
    "\n",
    "# grid.cv_results_   \n",
    "gridcv_lrt2.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=3.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adjust c range and rerun\n",
    "\n",
    "Cs=np.array([0.9,1,2,3])\n",
    "\n",
    "# fit model\n",
    "gridcv_lrt2=GridSearchCV(estimator=lrt2, param_grid=dict(C=Cs))\n",
    "gridcv_lrt2.fit(Xrf3,y)\n",
    "\n",
    "# grid.cv_results_   \n",
    "gridcv_lrt2.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=3, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adjust c range and rerun\n",
    "\n",
    "Cs=np.array([2,3,4])\n",
    "\n",
    "# fit model\n",
    "gridcv_lrt2=GridSearchCV(estimator=lrt2, param_grid=dict(C=Cs))\n",
    "gridcv_lrt2.fit(Xrf3,y)\n",
    "\n",
    "# grid.cv_results_   \n",
    "gridcv_lrt2.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# look at the results with C = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weights</th>\n",
       "      <th>Likelihood_Change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>quantitative</th>\n",
       "      <td>2.622498</td>\n",
       "      <td>13.770078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>machine</th>\n",
       "      <td>2.010368</td>\n",
       "      <td>7.466062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>director</th>\n",
       "      <td>1.751059</td>\n",
       "      <td>5.760698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>principal</th>\n",
       "      <td>1.523325</td>\n",
       "      <td>4.587455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data</th>\n",
       "      <td>1.426325</td>\n",
       "      <td>4.163371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lead</th>\n",
       "      <td>1.360396</td>\n",
       "      <td>3.897735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>architect</th>\n",
       "      <td>1.078796</td>\n",
       "      <td>2.941136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Philadelphia</th>\n",
       "      <td>1.052868</td>\n",
       "      <td>2.865858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sales</th>\n",
       "      <td>0.854563</td>\n",
       "      <td>2.350346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sr</th>\n",
       "      <td>0.747354</td>\n",
       "      <td>2.111406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scientist</th>\n",
       "      <td>0.717897</td>\n",
       "      <td>2.050116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>San Francisco</th>\n",
       "      <td>0.683111</td>\n",
       "      <td>1.980028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning</th>\n",
       "      <td>0.640356</td>\n",
       "      <td>1.897156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Austin</th>\n",
       "      <td>0.428548</td>\n",
       "      <td>1.535026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Seattle</th>\n",
       "      <td>0.346638</td>\n",
       "      <td>1.414305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>senior</th>\n",
       "      <td>0.346563</td>\n",
       "      <td>1.414199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>head</th>\n",
       "      <td>0.257747</td>\n",
       "      <td>1.294011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>engineer</th>\n",
       "      <td>0.257149</td>\n",
       "      <td>1.293238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>engineering</th>\n",
       "      <td>0.255636</td>\n",
       "      <td>1.291283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>risk</th>\n",
       "      <td>0.043620</td>\n",
       "      <td>1.044585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>science</th>\n",
       "      <td>0.011968</td>\n",
       "      <td>1.012040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>predictive</th>\n",
       "      <td>0.001982</td>\n",
       "      <td>1.001984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>overdose</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>outreach</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ospe</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Atlanta</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optimization</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>operations</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>open</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oncology</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entrepreneurship</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>end</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>earth</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enterprise</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>engineers</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>etl</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epidemiology</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>energy</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emergency</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>evaluation</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>experimental</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>facing</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elisa</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>editor</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Miami</th>\n",
       "      <td>-0.051794</td>\n",
       "      <td>0.949524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clinical</th>\n",
       "      <td>-0.132273</td>\n",
       "      <td>0.876102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>environmental</th>\n",
       "      <td>-0.168120</td>\n",
       "      <td>0.845253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phoenix</th>\n",
       "      <td>-0.246430</td>\n",
       "      <td>0.781586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medical</th>\n",
       "      <td>-0.286708</td>\n",
       "      <td>0.750731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New York</th>\n",
       "      <td>-0.295684</td>\n",
       "      <td>0.744023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>som</th>\n",
       "      <td>-0.524518</td>\n",
       "      <td>0.591841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bureau</th>\n",
       "      <td>-0.528521</td>\n",
       "      <td>0.589476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>assistant</th>\n",
       "      <td>-0.668407</td>\n",
       "      <td>0.512524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Portland</th>\n",
       "      <td>-0.713453</td>\n",
       "      <td>0.489949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>research</th>\n",
       "      <td>-0.828694</td>\n",
       "      <td>0.436619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>laboratory</th>\n",
       "      <td>-0.857494</td>\n",
       "      <td>0.424224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>specialist</th>\n",
       "      <td>-0.863126</td>\n",
       "      <td>0.421841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analyst</th>\n",
       "      <td>-0.878714</td>\n",
       "      <td>0.415317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>associate</th>\n",
       "      <td>-1.008921</td>\n",
       "      <td>0.364612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pittsburgh</th>\n",
       "      <td>-1.156518</td>\n",
       "      <td>0.314580</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>365 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   weights  Likelihood_Change\n",
       "quantitative      2.622498          13.770078\n",
       "machine           2.010368           7.466062\n",
       "director          1.751059           5.760698\n",
       "principal         1.523325           4.587455\n",
       "data              1.426325           4.163371\n",
       "lead              1.360396           3.897735\n",
       "architect         1.078796           2.941136\n",
       "Philadelphia      1.052868           2.865858\n",
       "sales             0.854563           2.350346\n",
       "sr                0.747354           2.111406\n",
       "scientist         0.717897           2.050116\n",
       "San Francisco     0.683111           1.980028\n",
       "learning          0.640356           1.897156\n",
       "Austin            0.428548           1.535026\n",
       "Seattle           0.346638           1.414305\n",
       "senior            0.346563           1.414199\n",
       "head              0.257747           1.294011\n",
       "engineer          0.257149           1.293238\n",
       "engineering       0.255636           1.291283\n",
       "risk              0.043620           1.044585\n",
       "science           0.011968           1.012040\n",
       "predictive        0.001982           1.001984\n",
       "overdose          0.000000           1.000000\n",
       "outreach          0.000000           1.000000\n",
       "ospe              0.000000           1.000000\n",
       "Atlanta           0.000000           1.000000\n",
       "optimization      0.000000           1.000000\n",
       "operations        0.000000           1.000000\n",
       "open              0.000000           1.000000\n",
       "oncology          0.000000           1.000000\n",
       "...                    ...                ...\n",
       "entrepreneurship  0.000000           1.000000\n",
       "end               0.000000           1.000000\n",
       "earth             0.000000           1.000000\n",
       "enterprise        0.000000           1.000000\n",
       "engineers         0.000000           1.000000\n",
       "etl               0.000000           1.000000\n",
       "epidemiology      0.000000           1.000000\n",
       "energy            0.000000           1.000000\n",
       "emergency         0.000000           1.000000\n",
       "evaluation        0.000000           1.000000\n",
       "experimental      0.000000           1.000000\n",
       "facing            0.000000           1.000000\n",
       "elisa             0.000000           1.000000\n",
       "editor            0.000000           1.000000\n",
       "Miami            -0.051794           0.949524\n",
       "clinical         -0.132273           0.876102\n",
       "environmental    -0.168120           0.845253\n",
       "Phoenix          -0.246430           0.781586\n",
       "medical          -0.286708           0.750731\n",
       "New York         -0.295684           0.744023\n",
       "som              -0.524518           0.591841\n",
       "bureau           -0.528521           0.589476\n",
       "assistant        -0.668407           0.512524\n",
       "Portland         -0.713453           0.489949\n",
       "research         -0.828694           0.436619\n",
       "laboratory       -0.857494           0.424224\n",
       "specialist       -0.863126           0.421841\n",
       "analyst          -0.878714           0.415317\n",
       "associate        -1.008921           0.364612\n",
       "Pittsburgh       -1.156518           0.314580\n",
       "\n",
       "[365 rows x 2 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Want to see associated coefficients into readable format\n",
    "\n",
    "coeff_valslrt2 = pd.DataFrame(lrt2.coef_.transpose(),index = Xrf3.columns,\n",
    "                                   columns=['weights']).sort_values('weights',\n",
    "                                                                       ascending=False)\n",
    "# Raise e to the coefficient column to get the likelihood of being above the median\n",
    "coeff_valslrt2['Likelihood_Change'] = np.exp(coeff_valslrt2['weights'])\n",
    "coeff_valslrt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.72881356  0.77966102  0.79661017  0.82758621  0.75438596]\n",
      "Linear Regression Score:\t0.777 ± 0.034\n"
     ]
    }
   ],
   "source": [
    "# Look at accuracy of the above model\n",
    "# Do cross validation and summarize the scores\n",
    "lrt2scores=cross_val_score(model_lrt2,Xrf3,y,cv=5)\n",
    "print lrt2scores\n",
    "print \"{} Score:\\t{:0.3} ± {:0.3}\".format(\"Linear Regression\", lrt2scores.mean().round(3), lrt2scores.std().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Similar accuracy to other models. \n",
    "# Will use words with non-zero coefficients after lasso regularization as good predictor words\n",
    "# Write words to file so can visualize in Tableau\n",
    "coeff_valslrt2.to_csv('final_title_keywords.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>quantitative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>machine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>director</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>principal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>scientist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>lead</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>architect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>senior</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>head</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>engineer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>clinical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>environmental</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>medical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>bureau</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>assistant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>research</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>laboratory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>specialist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>associate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Term\n",
       "0    quantitative\n",
       "1         machine\n",
       "2        learning\n",
       "3        director\n",
       "4       principal\n",
       "5            data\n",
       "6       scientist\n",
       "7            lead\n",
       "8       architect\n",
       "9           sales\n",
       "10         senior\n",
       "11           head\n",
       "12       engineer\n",
       "13       clinical\n",
       "14  environmental\n",
       "15        medical\n",
       "16         bureau\n",
       "17      assistant\n",
       "18       research\n",
       "19     laboratory\n",
       "20     specialist\n",
       "21        analyst\n",
       "22      associate"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Edited remaining keywords in csv file to those that made sense.   \n",
    "# Read edited file back in with 23 most impactful keywords from the title\n",
    "\n",
    "keywordsTitle=pd.read_csv('Title keywords to import.csv',header=0)\n",
    "keywordsTitle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.86440678  0.74576271  0.86440678  0.82758621  0.73684211]\n",
      "Linear Regression Score:\t0.808 ± 0.056\n"
     ]
    }
   ],
   "source": [
    "# Rerun logistic regression with just these title keywords (no location)\n",
    "\n",
    "XTitlekey = Xrf3[keywordsTitle['Term'].tolist()]    #X6 is reduced list of keywords\n",
    "lrTK = LogisticRegression()           \n",
    "model_lrTK= lrTK.fit(XTitlekey,y)\n",
    "\n",
    "# Do cross validation and summarize the scores\n",
    "lrTKscores=cross_val_score(model_lrTK,XTitlekey,y,cv=5)\n",
    "print lrTKscores\n",
    "print \"{} Score:\\t{:0.3} ± {:0.3}\".format(\"Linear Regression\", lrTKscores.mean().round(3), lrTKscores.std().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "80% accuracy, with some variance - about comparable to what have been seeing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weights</th>\n",
       "      <th>Likelihood_Change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>quantitative</th>\n",
       "      <td>1.936027</td>\n",
       "      <td>6.931155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>director</th>\n",
       "      <td>1.412827</td>\n",
       "      <td>4.107552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data</th>\n",
       "      <td>1.302226</td>\n",
       "      <td>3.677475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>machine</th>\n",
       "      <td>1.291843</td>\n",
       "      <td>3.639487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning</th>\n",
       "      <td>1.291843</td>\n",
       "      <td>3.639487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>principal</th>\n",
       "      <td>1.249448</td>\n",
       "      <td>3.488416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lead</th>\n",
       "      <td>1.128660</td>\n",
       "      <td>3.091511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sales</th>\n",
       "      <td>1.028853</td>\n",
       "      <td>2.797855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>architect</th>\n",
       "      <td>1.014450</td>\n",
       "      <td>2.757847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scientist</th>\n",
       "      <td>0.843835</td>\n",
       "      <td>2.325268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>head</th>\n",
       "      <td>0.841625</td>\n",
       "      <td>2.320133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>engineer</th>\n",
       "      <td>0.590463</td>\n",
       "      <td>1.804823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>senior</th>\n",
       "      <td>0.513961</td>\n",
       "      <td>1.671900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clinical</th>\n",
       "      <td>-0.612839</td>\n",
       "      <td>0.541810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analyst</th>\n",
       "      <td>-0.650553</td>\n",
       "      <td>0.521757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medical</th>\n",
       "      <td>-0.721457</td>\n",
       "      <td>0.486044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>environmental</th>\n",
       "      <td>-0.723144</td>\n",
       "      <td>0.485224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>laboratory</th>\n",
       "      <td>-0.867965</td>\n",
       "      <td>0.419805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>research</th>\n",
       "      <td>-0.887234</td>\n",
       "      <td>0.411793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bureau</th>\n",
       "      <td>-0.909180</td>\n",
       "      <td>0.402855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>specialist</th>\n",
       "      <td>-0.917424</td>\n",
       "      <td>0.399547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>associate</th>\n",
       "      <td>-0.951489</td>\n",
       "      <td>0.386165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>assistant</th>\n",
       "      <td>-1.012278</td>\n",
       "      <td>0.363390</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                weights  Likelihood_Change\n",
       "quantitative   1.936027           6.931155\n",
       "director       1.412827           4.107552\n",
       "data           1.302226           3.677475\n",
       "machine        1.291843           3.639487\n",
       "learning       1.291843           3.639487\n",
       "principal      1.249448           3.488416\n",
       "lead           1.128660           3.091511\n",
       "sales          1.028853           2.797855\n",
       "architect      1.014450           2.757847\n",
       "scientist      0.843835           2.325268\n",
       "head           0.841625           2.320133\n",
       "engineer       0.590463           1.804823\n",
       "senior         0.513961           1.671900\n",
       "clinical      -0.612839           0.541810\n",
       "analyst       -0.650553           0.521757\n",
       "medical       -0.721457           0.486044\n",
       "environmental -0.723144           0.485224\n",
       "laboratory    -0.867965           0.419805\n",
       "research      -0.887234           0.411793\n",
       "bureau        -0.909180           0.402855\n",
       "specialist    -0.917424           0.399547\n",
       "associate     -0.951489           0.386165\n",
       "assistant     -1.012278           0.363390"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add information on weights and likelihood\n",
    "coeff_valslrTK = pd.DataFrame(lrTK.coef_.transpose(),index = XTitlekey.columns,\n",
    "                                   columns=['weights']).sort_values('weights',\n",
    "                                                                       ascending=False)\n",
    "# Raise e to the coefficient column to get the likelihood of being above the median\n",
    "coeff_valslrTK['Likelihood_Change'] = np.exp(coeff_valslrTK['weights'])\n",
    "coeff_valslrTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write words to file so can visualize in Tableau\n",
    "coeff_valslrTK.to_csv('final_title_keywords_no_city.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "db045898-1d2d-4af2-8e79-437c4c7546b4"
   },
   "source": [
    "#### Repeat above for words in the job descriptions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Count vectorize the descriptions\n",
    "cvd=CountVectorizer(analyzer=\"word\",stop_words='english',decode_error='ignore')\n",
    "description_features = cvd.fit_transform(listings2.Description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(292, 1365)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index([     u'Atlanta',       u'Austin',      u'Chicago',       u'Dallas',\n",
       "             u'Denver',      u'Houston',  u'Los Angeles',        u'Miami',\n",
       "           u'New York', u'Philadelphia',\n",
       "       ...\n",
       "              u'works',        u'world',       u'worlds',        u'write',\n",
       "            u'writing',      u'written',         u'year',        u'years',\n",
       "               u'york',           u'yr'],\n",
       "      dtype='object', length=1365)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up the dependent variable input (X4) for the linear regression\n",
    "# Concatenate Metro (location) with the new features from the count vectorizer\n",
    "description_features_array = description_features.toarray()\n",
    "description_features_df = pd.DataFrame(description_features_array,columns=cvd.get_feature_names())\n",
    "\n",
    "X4 = pd.concat([pd.get_dummies(listings2.Metro),description_features_df],axis=1)\n",
    "print X4.shape\n",
    "X4.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.81355932  0.79661017  0.72881356  0.70689655  0.68421053]\n",
      "Logistic Regression Score:\t0.746 ± 0.051\n"
     ]
    }
   ],
   "source": [
    "# Do logistic regression including all the title words identified by the count vectorizer\n",
    "lr4 = LogisticRegression()      \n",
    "model_lr4= lr4.fit(X4,y)\n",
    "\n",
    "# Do cross validation and summarize the scores\n",
    "lr4scores=cross_val_score(model_lr4,X4,y,cv=5)\n",
    "print lr4scores\n",
    "print \"{} Score:\\t{:0.3} ± {:0.3}\".format(\"Logistic Regression\", lr4scores.mean().round(3), lr4scores.std().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Accuracy is down and variance is up with this model vs one based just on the title.   \n",
    "However we can use it to infer some words that might be impactful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weights</th>\n",
       "      <th>Likelihood_Change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>looking</th>\n",
       "      <td>1.077197</td>\n",
       "      <td>2.936437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>San Francisco</th>\n",
       "      <td>0.970103</td>\n",
       "      <td>2.638216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analytical</th>\n",
       "      <td>0.929447</td>\n",
       "      <td>2.533107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>senior</th>\n",
       "      <td>0.909150</td>\n",
       "      <td>2.482211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>modeling</th>\n",
       "      <td>0.779938</td>\n",
       "      <td>2.181336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leading</th>\n",
       "      <td>0.778612</td>\n",
       "      <td>2.178446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>join</th>\n",
       "      <td>0.767944</td>\n",
       "      <td>2.155330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <td>0.698322</td>\n",
       "      <td>2.010377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Philadelphia</th>\n",
       "      <td>0.696852</td>\n",
       "      <td>2.007424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>team</th>\n",
       "      <td>0.681965</td>\n",
       "      <td>1.977761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>role</th>\n",
       "      <td>0.662570</td>\n",
       "      <td>1.939771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>field</th>\n",
       "      <td>0.645892</td>\n",
       "      <td>1.907689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>challenges</th>\n",
       "      <td>0.633813</td>\n",
       "      <td>1.884784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>teams</th>\n",
       "      <td>0.605628</td>\n",
       "      <td>1.832403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>predictive</th>\n",
       "      <td>0.578380</td>\n",
       "      <td>1.783147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>understand</th>\n",
       "      <td>0.577934</td>\n",
       "      <td>1.782351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>science</th>\n",
       "      <td>0.572278</td>\n",
       "      <td>1.772299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>statistician</th>\n",
       "      <td>0.569723</td>\n",
       "      <td>1.767776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>company</th>\n",
       "      <td>0.552971</td>\n",
       "      <td>1.738410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>building</th>\n",
       "      <td>0.543711</td>\n",
       "      <td>1.722387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new</th>\n",
       "      <td>0.534806</td>\n",
       "      <td>1.707117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scientist</th>\n",
       "      <td>0.534374</td>\n",
       "      <td>1.706380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>questions</th>\n",
       "      <td>0.513953</td>\n",
       "      <td>1.671886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Austin</th>\n",
       "      <td>0.512220</td>\n",
       "      <td>1.668992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trial</th>\n",
       "      <td>0.501222</td>\n",
       "      <td>1.650738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>biological</th>\n",
       "      <td>0.499566</td>\n",
       "      <td>1.648006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>standards</th>\n",
       "      <td>0.483721</td>\n",
       "      <td>1.622100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>innovative</th>\n",
       "      <td>0.483354</td>\n",
       "      <td>1.621505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>growing</th>\n",
       "      <td>0.483038</td>\n",
       "      <td>1.620992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lead</th>\n",
       "      <td>0.464318</td>\n",
       "      <td>1.590929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>student</th>\n",
       "      <td>-0.397926</td>\n",
       "      <td>0.671711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>extensive</th>\n",
       "      <td>-0.398494</td>\n",
       "      <td>0.671330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>computer</th>\n",
       "      <td>-0.399696</td>\n",
       "      <td>0.670524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>assist</th>\n",
       "      <td>-0.402028</td>\n",
       "      <td>0.668962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qualitative</th>\n",
       "      <td>-0.404700</td>\n",
       "      <td>0.667177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phoenix</th>\n",
       "      <td>-0.411394</td>\n",
       "      <td>0.662726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>applied</th>\n",
       "      <td>-0.412400</td>\n",
       "      <td>0.662059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>assistant</th>\n",
       "      <td>-0.413099</td>\n",
       "      <td>0.661597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>legal</th>\n",
       "      <td>-0.431394</td>\n",
       "      <td>0.649603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>develop</th>\n",
       "      <td>-0.442658</td>\n",
       "      <td>0.642327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pipelines</th>\n",
       "      <td>-0.442675</td>\n",
       "      <td>0.642316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>toolset</th>\n",
       "      <td>-0.463588</td>\n",
       "      <td>0.629023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>assists</th>\n",
       "      <td>-0.473532</td>\n",
       "      <td>0.622799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sente</th>\n",
       "      <td>-0.477864</td>\n",
       "      <td>0.620106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Miami</th>\n",
       "      <td>-0.487346</td>\n",
       "      <td>0.614254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review</th>\n",
       "      <td>-0.518946</td>\n",
       "      <td>0.595147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analyze</th>\n",
       "      <td>-0.519817</td>\n",
       "      <td>0.594629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>project</th>\n",
       "      <td>-0.553254</td>\n",
       "      <td>0.575075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seeks</th>\n",
       "      <td>-0.564019</td>\n",
       "      <td>0.568918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clinical</th>\n",
       "      <td>-0.591646</td>\n",
       "      <td>0.553416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>environmental</th>\n",
       "      <td>-0.623467</td>\n",
       "      <td>0.536083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analyst</th>\n",
       "      <td>-0.634465</td>\n",
       "      <td>0.530219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>including</th>\n",
       "      <td>-0.691852</td>\n",
       "      <td>0.500648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>staff</th>\n",
       "      <td>-0.704202</td>\n",
       "      <td>0.494503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reports</th>\n",
       "      <td>-0.713301</td>\n",
       "      <td>0.490024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Portland</th>\n",
       "      <td>-0.768490</td>\n",
       "      <td>0.463713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>research</th>\n",
       "      <td>-0.785875</td>\n",
       "      <td>0.455721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>laboratory</th>\n",
       "      <td>-0.799914</td>\n",
       "      <td>0.449368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pittsburgh</th>\n",
       "      <td>-0.818704</td>\n",
       "      <td>0.441003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality</th>\n",
       "      <td>-0.940877</td>\n",
       "      <td>0.390285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1365 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                weights  Likelihood_Change\n",
       "looking        1.077197           2.936437\n",
       "San Francisco  0.970103           2.638216\n",
       "analytical     0.929447           2.533107\n",
       "senior         0.909150           2.482211\n",
       "modeling       0.779938           2.181336\n",
       "leading        0.778612           2.178446\n",
       "join           0.767944           2.155330\n",
       "model          0.698322           2.010377\n",
       "Philadelphia   0.696852           2.007424\n",
       "team           0.681965           1.977761\n",
       "role           0.662570           1.939771\n",
       "field          0.645892           1.907689\n",
       "challenges     0.633813           1.884784\n",
       "teams          0.605628           1.832403\n",
       "predictive     0.578380           1.783147\n",
       "understand     0.577934           1.782351\n",
       "science        0.572278           1.772299\n",
       "statistician   0.569723           1.767776\n",
       "company        0.552971           1.738410\n",
       "building       0.543711           1.722387\n",
       "new            0.534806           1.707117\n",
       "scientist      0.534374           1.706380\n",
       "questions      0.513953           1.671886\n",
       "Austin         0.512220           1.668992\n",
       "trial          0.501222           1.650738\n",
       "biological     0.499566           1.648006\n",
       "standards      0.483721           1.622100\n",
       "innovative     0.483354           1.621505\n",
       "growing        0.483038           1.620992\n",
       "lead           0.464318           1.590929\n",
       "...                 ...                ...\n",
       "student       -0.397926           0.671711\n",
       "extensive     -0.398494           0.671330\n",
       "computer      -0.399696           0.670524\n",
       "assist        -0.402028           0.668962\n",
       "qualitative   -0.404700           0.667177\n",
       "Phoenix       -0.411394           0.662726\n",
       "applied       -0.412400           0.662059\n",
       "assistant     -0.413099           0.661597\n",
       "legal         -0.431394           0.649603\n",
       "develop       -0.442658           0.642327\n",
       "pipelines     -0.442675           0.642316\n",
       "toolset       -0.463588           0.629023\n",
       "assists       -0.473532           0.622799\n",
       "sente         -0.477864           0.620106\n",
       "Miami         -0.487346           0.614254\n",
       "review        -0.518946           0.595147\n",
       "analyze       -0.519817           0.594629\n",
       "project       -0.553254           0.575075\n",
       "seeks         -0.564019           0.568918\n",
       "clinical      -0.591646           0.553416\n",
       "environmental -0.623467           0.536083\n",
       "analyst       -0.634465           0.530219\n",
       "including     -0.691852           0.500648\n",
       "staff         -0.704202           0.494503\n",
       "reports       -0.713301           0.490024\n",
       "Portland      -0.768490           0.463713\n",
       "research      -0.785875           0.455721\n",
       "laboratory    -0.799914           0.449368\n",
       "Pittsburgh    -0.818704           0.441003\n",
       "quality       -0.940877           0.390285\n",
       "\n",
       "[1365 rows x 2 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put coefficients into readable format\n",
    "coeff_vals4 = pd.DataFrame(lr4.coef_.transpose(),index = X4.columns,\n",
    "                                   columns=['weights']).sort_values('weights',\n",
    "                                                                       ascending=False)\n",
    "# Raise e to the coefficient column to get the likelihood of being above the median\n",
    "coeff_vals4['Likelihood_Change'] = np.exp(coeff_vals4['weights'])\n",
    "coeff_vals4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "With Grid search rerun logistic regression with L1 regularization and range of C (inverse of alpha) to try to determine key variables in description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use features X4 from above (Metro/location + all words from the description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#setup parameters\n",
    "lr5 = LogisticRegression(penalty='l1')      \n",
    "model_lr5= lr5.fit(X4,y)\n",
    "\n",
    "Cs=np.array([0.001,0.01,0.1,1])\n",
    "\n",
    "gridcv_lr5=GridSearchCV(estimator=lr5, param_grid=dict(C=Cs))\n",
    "gridcv_lr5.fit(X4,y)\n",
    "\n",
    "# grid.cv_results_   \n",
    "gridcv_lr5.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best C was 1.0.  Will rerun with aditional values\n",
    "Cs=np.array([0.5,0.8,1,2,10])\n",
    "\n",
    "gridcv_lr5=GridSearchCV(estimator=lr5, param_grid=dict(C=Cs))\n",
    "gridcv_lr5.fit(X4,y)\n",
    "\n",
    "# grid.cv_results_   #not very interpretable output\n",
    "gridcv_lr5.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=8, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best C was 10.  Will rerun with aditional values\n",
    "Cs=np.array([8,10,15,20])\n",
    "\n",
    "gridcv_lr5=GridSearchCV(estimator=lr5, param_grid=dict(C=Cs))\n",
    "gridcv_lr5.fit(X4,y)\n",
    "\n",
    "# grid.cv_results_   #not very interpretable output\n",
    "gridcv_lr5.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=5, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Best C was 8. Will rerun to hone in\n",
    "Cs=np.array([5,6,7,8,9,10])\n",
    "\n",
    "gridcv_lr5=GridSearchCV(estimator=lr5, param_grid=dict(C=Cs))\n",
    "gridcv_lr5.fit(X4,y)\n",
    "\n",
    "# grid.cv_results_   #not very interpretable output\n",
    "gridcv_lr5.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>leading</th>\n",
       "      <td>1.886989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>looking</th>\n",
       "      <td>1.840003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>challenges</th>\n",
       "      <td>1.798919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>San Francisco</th>\n",
       "      <td>1.736509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analytical</th>\n",
       "      <td>1.525006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>modeling</th>\n",
       "      <td>1.450353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>statistician</th>\n",
       "      <td>1.401144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <td>1.400228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>senior</th>\n",
       "      <td>1.301459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>questions</th>\n",
       "      <td>1.281091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>join</th>\n",
       "      <td>1.241459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>field</th>\n",
       "      <td>1.209633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Philadelphia</th>\n",
       "      <td>1.136807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Austin</th>\n",
       "      <td>0.916381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning</th>\n",
       "      <td>0.846097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>predictive</th>\n",
       "      <td>0.842741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>role</th>\n",
       "      <td>0.828435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>innovative</th>\n",
       "      <td>0.791669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trial</th>\n",
       "      <td>0.772176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scientist</th>\n",
       "      <td>0.717573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>team</th>\n",
       "      <td>0.705564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>teams</th>\n",
       "      <td>0.671284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>standards</th>\n",
       "      <td>0.670203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>biological</th>\n",
       "      <td>0.646734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product</th>\n",
       "      <td>0.597140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>company</th>\n",
       "      <td>0.558562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>client</th>\n",
       "      <td>0.533495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>models</th>\n",
       "      <td>0.452070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>science</th>\n",
       "      <td>0.421793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>small</th>\n",
       "      <td>0.400550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>working</th>\n",
       "      <td>-0.028368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>legal</th>\n",
       "      <td>-0.030790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>assist</th>\n",
       "      <td>-0.057238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analysis</th>\n",
       "      <td>-0.064705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Atlanta</th>\n",
       "      <td>-0.072884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seeks</th>\n",
       "      <td>-0.088318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analyze</th>\n",
       "      <td>-0.103363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>management</th>\n",
       "      <td>-0.146957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analyst</th>\n",
       "      <td>-0.238876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>develop</th>\n",
       "      <td>-0.255083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>using</th>\n",
       "      <td>-0.256045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>development</th>\n",
       "      <td>-0.282863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>program</th>\n",
       "      <td>-0.314528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>assistant</th>\n",
       "      <td>-0.336157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>computer</th>\n",
       "      <td>-0.342232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qualitative</th>\n",
       "      <td>-0.344389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review</th>\n",
       "      <td>-0.408733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clinical</th>\n",
       "      <td>-0.422231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>financial</th>\n",
       "      <td>-0.499389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>environmental</th>\n",
       "      <td>-0.539759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Miami</th>\n",
       "      <td>-0.581418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>research</th>\n",
       "      <td>-0.649253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>project</th>\n",
       "      <td>-0.671480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>staff</th>\n",
       "      <td>-0.716271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>including</th>\n",
       "      <td>-0.731002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Portland</th>\n",
       "      <td>-0.910657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reports</th>\n",
       "      <td>-0.915207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>laboratory</th>\n",
       "      <td>-1.182360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pittsburgh</th>\n",
       "      <td>-1.249595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality</th>\n",
       "      <td>-1.705622</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1365 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                weights\n",
       "leading        1.886989\n",
       "looking        1.840003\n",
       "challenges     1.798919\n",
       "San Francisco  1.736509\n",
       "analytical     1.525006\n",
       "modeling       1.450353\n",
       "statistician   1.401144\n",
       "model          1.400228\n",
       "senior         1.301459\n",
       "questions      1.281091\n",
       "join           1.241459\n",
       "field          1.209633\n",
       "Philadelphia   1.136807\n",
       "Austin         0.916381\n",
       "learning       0.846097\n",
       "predictive     0.842741\n",
       "role           0.828435\n",
       "innovative     0.791669\n",
       "trial          0.772176\n",
       "scientist      0.717573\n",
       "team           0.705564\n",
       "teams          0.671284\n",
       "standards      0.670203\n",
       "biological     0.646734\n",
       "product        0.597140\n",
       "company        0.558562\n",
       "client         0.533495\n",
       "models         0.452070\n",
       "science        0.421793\n",
       "small          0.400550\n",
       "...                 ...\n",
       "working       -0.028368\n",
       "legal         -0.030790\n",
       "assist        -0.057238\n",
       "analysis      -0.064705\n",
       "Atlanta       -0.072884\n",
       "seeks         -0.088318\n",
       "analyze       -0.103363\n",
       "management    -0.146957\n",
       "analyst       -0.238876\n",
       "develop       -0.255083\n",
       "using         -0.256045\n",
       "development   -0.282863\n",
       "program       -0.314528\n",
       "assistant     -0.336157\n",
       "computer      -0.342232\n",
       "qualitative   -0.344389\n",
       "review        -0.408733\n",
       "clinical      -0.422231\n",
       "financial     -0.499389\n",
       "environmental -0.539759\n",
       "Miami         -0.581418\n",
       "research      -0.649253\n",
       "project       -0.671480\n",
       "staff         -0.716271\n",
       "including     -0.731002\n",
       "Portland      -0.910657\n",
       "reports       -0.915207\n",
       "laboratory    -1.182360\n",
       "Pittsburgh    -1.249595\n",
       "quality       -1.705622\n",
       "\n",
       "[1365 rows x 1 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best C was 6.  Want to see associated coefficients into readable format\n",
    "coeff_vals5 = pd.DataFrame(lr5.coef_.transpose(),index = X4.columns,\n",
    "                                   columns=['weights']).sort_values('weights',\n",
    "                                                                       ascending=False)\n",
    "coeff_vals5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Look at how many weights driven to 0\n",
    "# coeff_vals5[coeff_vals5.weights==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1283 out 1365 weights driven to 0\n",
    "# Look at weights that are not 0\n",
    "# coeff_vals5[coeff_vals5.weights!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write these to csv, manually remove those that I think are not meaningful \n",
    "# (like 'role') and then read back in file to work from going forward\n",
    "coeff_vals5.to_csv('key features.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "These cities had coefficients driven to 0:\n",
    "Seattle\n",
    "Raleigh\n",
    "Phoenix\n",
    "New York\n",
    "Los Angeles\n",
    "Houston\n",
    "Denver\n",
    "Dallas\n",
    "Interpreting that their salaries are near median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read edited file back in with 40 most impactful keywords from the description plus 8 impactful cities\n",
    "keywords=pd.read_csv('key features2.csv',header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.77966102  0.77966102  0.77966102  0.77586207  0.73684211]\n",
      "Linear Regression Score:\t0.77 ± 0.017\n"
     ]
    }
   ],
   "source": [
    "# Rerun logistic regression to get new weights \n",
    "X6 = X4[keywords['Unnamed: 0'].tolist()]    #X6 is reduced list of keywords\n",
    "lr6 = LogisticRegression()           \n",
    "model_lr6= lr6.fit(X6,y)\n",
    "\n",
    "# Do cross validation and summarize the scores\n",
    "lr6scores=cross_val_score(model_lr6,X6,y,cv=5)\n",
    "print lr6scores\n",
    "print \"{} Score:\\t{:0.3} ± {:0.3}\".format(\"Linear Regression\", lr6scores.mean().round(3), lr6scores.std().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weights</th>\n",
       "      <th>Likelihood_Change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>San Francisco</th>\n",
       "      <td>1.567672</td>\n",
       "      <td>4.795472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leading</th>\n",
       "      <td>1.326212</td>\n",
       "      <td>3.766747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analytical</th>\n",
       "      <td>1.314655</td>\n",
       "      <td>3.723466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>innovative</th>\n",
       "      <td>1.279602</td>\n",
       "      <td>3.595208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Philadelphia</th>\n",
       "      <td>1.251906</td>\n",
       "      <td>3.497001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>challenges</th>\n",
       "      <td>1.221201</td>\n",
       "      <td>3.391258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>field</th>\n",
       "      <td>1.181571</td>\n",
       "      <td>3.259492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>senior</th>\n",
       "      <td>1.178358</td>\n",
       "      <td>3.249035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <td>1.173834</td>\n",
       "      <td>3.234370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>modeling</th>\n",
       "      <td>1.120743</td>\n",
       "      <td>3.067132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Austin</th>\n",
       "      <td>1.095414</td>\n",
       "      <td>2.990422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>client</th>\n",
       "      <td>1.077403</td>\n",
       "      <td>2.937043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>statistician</th>\n",
       "      <td>1.027593</td>\n",
       "      <td>2.794332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>predictive</th>\n",
       "      <td>1.014237</td>\n",
       "      <td>2.757259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>biological</th>\n",
       "      <td>0.901893</td>\n",
       "      <td>2.464264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scientist</th>\n",
       "      <td>0.881818</td>\n",
       "      <td>2.415286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>science</th>\n",
       "      <td>0.795483</td>\n",
       "      <td>2.215510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>programming</th>\n",
       "      <td>0.750179</td>\n",
       "      <td>2.117378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>models</th>\n",
       "      <td>0.683291</td>\n",
       "      <td>1.980385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scientists</th>\n",
       "      <td>0.676004</td>\n",
       "      <td>1.966006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chicago</th>\n",
       "      <td>0.537177</td>\n",
       "      <td>1.711170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lead</th>\n",
       "      <td>0.525184</td>\n",
       "      <td>1.690770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning</th>\n",
       "      <td>0.397299</td>\n",
       "      <td>1.487801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mining</th>\n",
       "      <td>0.386015</td>\n",
       "      <td>1.471107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>machine</th>\n",
       "      <td>0.190181</td>\n",
       "      <td>1.209469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data</th>\n",
       "      <td>0.128328</td>\n",
       "      <td>1.136925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Atlanta</th>\n",
       "      <td>-0.143545</td>\n",
       "      <td>0.866282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analysis</th>\n",
       "      <td>-0.221738</td>\n",
       "      <td>0.801125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>management</th>\n",
       "      <td>-0.396417</td>\n",
       "      <td>0.672726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analyst</th>\n",
       "      <td>-0.412123</td>\n",
       "      <td>0.662243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>develop</th>\n",
       "      <td>-0.457696</td>\n",
       "      <td>0.632740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>program</th>\n",
       "      <td>-0.551333</td>\n",
       "      <td>0.576181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clinical</th>\n",
       "      <td>-0.562939</td>\n",
       "      <td>0.569533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>assistant</th>\n",
       "      <td>-0.657220</td>\n",
       "      <td>0.518290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qualitative</th>\n",
       "      <td>-0.664452</td>\n",
       "      <td>0.514556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>development</th>\n",
       "      <td>-0.680546</td>\n",
       "      <td>0.506341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>research</th>\n",
       "      <td>-0.757383</td>\n",
       "      <td>0.468892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>assist</th>\n",
       "      <td>-0.789272</td>\n",
       "      <td>0.454175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Miami</th>\n",
       "      <td>-0.802342</td>\n",
       "      <td>0.448278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analyze</th>\n",
       "      <td>-0.841900</td>\n",
       "      <td>0.430891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>financial</th>\n",
       "      <td>-0.854292</td>\n",
       "      <td>0.425584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>staff</th>\n",
       "      <td>-0.879677</td>\n",
       "      <td>0.414917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Portland</th>\n",
       "      <td>-0.880104</td>\n",
       "      <td>0.414740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>environmental</th>\n",
       "      <td>-0.925335</td>\n",
       "      <td>0.396398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>project</th>\n",
       "      <td>-0.959845</td>\n",
       "      <td>0.382952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>laboratory</th>\n",
       "      <td>-1.128869</td>\n",
       "      <td>0.323399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pittsburgh</th>\n",
       "      <td>-1.333330</td>\n",
       "      <td>0.263598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality</th>\n",
       "      <td>-1.541422</td>\n",
       "      <td>0.214077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                weights  Likelihood_Change\n",
       "San Francisco  1.567672           4.795472\n",
       "leading        1.326212           3.766747\n",
       "analytical     1.314655           3.723466\n",
       "innovative     1.279602           3.595208\n",
       "Philadelphia   1.251906           3.497001\n",
       "challenges     1.221201           3.391258\n",
       "field          1.181571           3.259492\n",
       "senior         1.178358           3.249035\n",
       "model          1.173834           3.234370\n",
       "modeling       1.120743           3.067132\n",
       "Austin         1.095414           2.990422\n",
       "client         1.077403           2.937043\n",
       "statistician   1.027593           2.794332\n",
       "predictive     1.014237           2.757259\n",
       "biological     0.901893           2.464264\n",
       "scientist      0.881818           2.415286\n",
       "science        0.795483           2.215510\n",
       "programming    0.750179           2.117378\n",
       "models         0.683291           1.980385\n",
       "scientists     0.676004           1.966006\n",
       "Chicago        0.537177           1.711170\n",
       "lead           0.525184           1.690770\n",
       "learning       0.397299           1.487801\n",
       "mining         0.386015           1.471107\n",
       "machine        0.190181           1.209469\n",
       "data           0.128328           1.136925\n",
       "Atlanta       -0.143545           0.866282\n",
       "analysis      -0.221738           0.801125\n",
       "management    -0.396417           0.672726\n",
       "analyst       -0.412123           0.662243\n",
       "develop       -0.457696           0.632740\n",
       "program       -0.551333           0.576181\n",
       "clinical      -0.562939           0.569533\n",
       "assistant     -0.657220           0.518290\n",
       "qualitative   -0.664452           0.514556\n",
       "development   -0.680546           0.506341\n",
       "research      -0.757383           0.468892\n",
       "assist        -0.789272           0.454175\n",
       "Miami         -0.802342           0.448278\n",
       "analyze       -0.841900           0.430891\n",
       "financial     -0.854292           0.425584\n",
       "staff         -0.879677           0.414917\n",
       "Portland      -0.880104           0.414740\n",
       "environmental -0.925335           0.396398\n",
       "project       -0.959845           0.382952\n",
       "laboratory    -1.128869           0.323399\n",
       "Pittsburgh    -1.333330           0.263598\n",
       "quality       -1.541422           0.214077"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Similar to best scores I had previously\n",
    "# Look at associated coefficients into readable format\n",
    "coeff_vals6 = pd.DataFrame(lr6.coef_.transpose(),index = X6.columns,\n",
    "                                   columns=['weights']).sort_values('weights',\n",
    "                                                                       ascending=False)\n",
    "# Raise e to the coefficient column to get the likelihood of being above the median\n",
    "coeff_vals6['Likelihood_Change'] = np.exp(coeff_vals6['weights'])\n",
    "coeff_vals6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write the words and weights to a csv file so key feed to Tableau\n",
    "coeff_vals6.to_csv('final_keyword_values.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Redo above without the cities - want key words from description only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Edit previous file and read back in\n",
    "keywordsD=pd.read_csv('key features3.csv',header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.83050847  0.74576271  0.79661017  0.74137931  0.75438596]\n",
      "Linear Regression Score:\t0.774 ± 0.034\n"
     ]
    }
   ],
   "source": [
    "# Rerun logistic regression to get new weights \n",
    "XkD = X4[keywordsD['Unnamed: 0'].tolist()]    #X6 is reduced list of keywords\n",
    "lrkD = LogisticRegression()           \n",
    "model_lrkD= lrkD.fit(XkD,y)\n",
    "\n",
    "# Do cross validation and summarize the scores\n",
    "lrkDscores=cross_val_score(model_lrkD,XkD,y,cv=5)\n",
    "print lrkDscores\n",
    "print \"{} Score:\\t{:0.3} ± {:0.3}\".format(\"Linear Regression\", lrkDscores.mean().round(3), lrkDscores.std().round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Similar accuracy and variance to other regressions and decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weights</th>\n",
       "      <th>Likelihood_Change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>leading</th>\n",
       "      <td>1.497124</td>\n",
       "      <td>4.468818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>innovative</th>\n",
       "      <td>1.420555</td>\n",
       "      <td>4.139417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <td>1.291245</td>\n",
       "      <td>3.637311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analytical</th>\n",
       "      <td>1.214273</td>\n",
       "      <td>3.367845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>client</th>\n",
       "      <td>1.174371</td>\n",
       "      <td>3.236106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>modeling</th>\n",
       "      <td>1.172807</td>\n",
       "      <td>3.231050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>challenges</th>\n",
       "      <td>1.155338</td>\n",
       "      <td>3.175097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>senior</th>\n",
       "      <td>1.116352</td>\n",
       "      <td>3.053693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>field</th>\n",
       "      <td>1.106593</td>\n",
       "      <td>3.024037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>statistician</th>\n",
       "      <td>1.012160</td>\n",
       "      <td>2.751538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>predictive</th>\n",
       "      <td>0.998985</td>\n",
       "      <td>2.715524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>biological</th>\n",
       "      <td>0.848101</td>\n",
       "      <td>2.335209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scientist</th>\n",
       "      <td>0.809137</td>\n",
       "      <td>2.245969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>programming</th>\n",
       "      <td>0.795409</td>\n",
       "      <td>2.215348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>science</th>\n",
       "      <td>0.738115</td>\n",
       "      <td>2.091989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning</th>\n",
       "      <td>0.706663</td>\n",
       "      <td>2.027216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>models</th>\n",
       "      <td>0.694938</td>\n",
       "      <td>2.003585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lead</th>\n",
       "      <td>0.575331</td>\n",
       "      <td>1.777719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scientists</th>\n",
       "      <td>0.564239</td>\n",
       "      <td>1.758110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>machine</th>\n",
       "      <td>0.384477</td>\n",
       "      <td>1.468846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mining</th>\n",
       "      <td>0.332214</td>\n",
       "      <td>1.394051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data</th>\n",
       "      <td>0.141668</td>\n",
       "      <td>1.152194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>management</th>\n",
       "      <td>-0.192701</td>\n",
       "      <td>0.824729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analysis</th>\n",
       "      <td>-0.234886</td>\n",
       "      <td>0.790661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analyst</th>\n",
       "      <td>-0.409350</td>\n",
       "      <td>0.664082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>development</th>\n",
       "      <td>-0.411870</td>\n",
       "      <td>0.662410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clinical</th>\n",
       "      <td>-0.500370</td>\n",
       "      <td>0.606307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>develop</th>\n",
       "      <td>-0.533633</td>\n",
       "      <td>0.586470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>program</th>\n",
       "      <td>-0.653810</td>\n",
       "      <td>0.520061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qualitative</th>\n",
       "      <td>-0.684784</td>\n",
       "      <td>0.504199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>research</th>\n",
       "      <td>-0.715130</td>\n",
       "      <td>0.489128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>financial</th>\n",
       "      <td>-0.799869</td>\n",
       "      <td>0.449388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analyze</th>\n",
       "      <td>-0.818001</td>\n",
       "      <td>0.441313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>assist</th>\n",
       "      <td>-0.861247</td>\n",
       "      <td>0.422635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>assistant</th>\n",
       "      <td>-0.861300</td>\n",
       "      <td>0.422613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>environmental</th>\n",
       "      <td>-0.897089</td>\n",
       "      <td>0.407755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>project</th>\n",
       "      <td>-0.995496</td>\n",
       "      <td>0.369540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>laboratory</th>\n",
       "      <td>-1.062657</td>\n",
       "      <td>0.345536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>staff</th>\n",
       "      <td>-1.093788</td>\n",
       "      <td>0.334945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality</th>\n",
       "      <td>-1.645439</td>\n",
       "      <td>0.192928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                weights  Likelihood_Change\n",
       "leading        1.497124           4.468818\n",
       "innovative     1.420555           4.139417\n",
       "model          1.291245           3.637311\n",
       "analytical     1.214273           3.367845\n",
       "client         1.174371           3.236106\n",
       "modeling       1.172807           3.231050\n",
       "challenges     1.155338           3.175097\n",
       "senior         1.116352           3.053693\n",
       "field          1.106593           3.024037\n",
       "statistician   1.012160           2.751538\n",
       "predictive     0.998985           2.715524\n",
       "biological     0.848101           2.335209\n",
       "scientist      0.809137           2.245969\n",
       "programming    0.795409           2.215348\n",
       "science        0.738115           2.091989\n",
       "learning       0.706663           2.027216\n",
       "models         0.694938           2.003585\n",
       "lead           0.575331           1.777719\n",
       "scientists     0.564239           1.758110\n",
       "machine        0.384477           1.468846\n",
       "mining         0.332214           1.394051\n",
       "data           0.141668           1.152194\n",
       "management    -0.192701           0.824729\n",
       "analysis      -0.234886           0.790661\n",
       "analyst       -0.409350           0.664082\n",
       "development   -0.411870           0.662410\n",
       "clinical      -0.500370           0.606307\n",
       "develop       -0.533633           0.586470\n",
       "program       -0.653810           0.520061\n",
       "qualitative   -0.684784           0.504199\n",
       "research      -0.715130           0.489128\n",
       "financial     -0.799869           0.449388\n",
       "analyze       -0.818001           0.441313\n",
       "assist        -0.861247           0.422635\n",
       "assistant     -0.861300           0.422613\n",
       "environmental -0.897089           0.407755\n",
       "project       -0.995496           0.369540\n",
       "laboratory    -1.062657           0.345536\n",
       "staff         -1.093788           0.334945\n",
       "quality       -1.645439           0.192928"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at associated coefficients into readable format\n",
    "coeff_valskD = pd.DataFrame(lrkD.coef_.transpose(),index = XkD.columns,\n",
    "                                   columns=['weights']).sort_values('weights',\n",
    "                                                                       ascending=False)\n",
    "# Raise e to the coefficient column to get the likelihood of being above the median\n",
    "coeff_valskD['Likelihood_Change'] = np.exp(coeff_valskD['weights'])\n",
    "coeff_valskD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write the words and weights to a csv file so can feed to Tableau\n",
    "coeff_valskD.to_csv('final_Description_values.csv')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
